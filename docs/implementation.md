# Second Brain - Implementation Log

## Overview
Real-time meeting assistant with contextual intelligence built with Tauri v2 + Svelte 5.

---

## Session 1 - December 1, 2025

### What Was Built

#### Phase 1: Project Foundation
- Initialized Tauri v2 + Svelte 5 project in `/app` directory
- Package manager: Bun
- Configured `tauri.conf.json` with proper app name and settings
- Added system tray configuration

#### Files Created/Modified
- `/app/src-tauri/Cargo.toml` - Rust dependencies (tauri with tray-icon, tokio)
- `/app/src-tauri/tauri.conf.json` - App config, tray icon, window settings
- `/app/src-tauri/src/lib.rs` - Rust backend with:
  - System tray menu (Show, Start Recording, Stop Recording, Quit)
  - Recording state management
  - IPC commands (start_recording, stop_recording, is_recording)
- `/app/src/routes/+page.svelte` - Main UI with:
  - Recording status indicator
  - Start/Stop recording button
  - Minimal/clean design with dark mode support

### How It Works

#### Architecture
```
Tauri App
â”œâ”€â”€ Frontend (Svelte 5 in webview)
â”‚   â”œâ”€â”€ UI components
â”‚   â””â”€â”€ Calls Rust via invoke()
â””â”€â”€ Backend (Rust)
    â”œâ”€â”€ System tray management
    â”œâ”€â”€ Recording state (AtomicBool)
    â””â”€â”€ Tauri commands exposed to frontend
```

#### System Tray
- Icon appears in macOS menu bar
- Right-click menu: Show Window, Start Recording, Stop Recording, Quit
- Left-click: Show main window

#### Recording State
- Managed via `std::sync::atomic::AtomicBool` for thread safety
- Frontend polls via `is_recording` command
- Can be toggled from tray menu OR main window

### Errors & Solutions

#### Error 1: Directory not empty
```
Directory is not empty, Operation Cancelled
```
**Solution:** Created project in `/app` subdirectory instead of root (which had prompt.md)

#### Error 2: Bad auto-generated names
Tauri used full path as app name: `userssubhankarchowdhurydevfoldersecond-brainapp`
**Solution:** Manually edited Cargo.toml and tauri.conf.json to use proper names

#### Error 3: main.rs had wrong library name
```
error[E0433]: failed to resolve: use of unresolved module or unlinked crate `userssubhankarchowdhurydevfoldersecond_brainapp_lib`
```
**Solution:** Fixed `src-tauri/src/main.rs` to use `second_brain_lib::run()` instead of the auto-generated ugly name

### Tech Stack Decisions

| Component | Choice | Rationale |
|-----------|--------|-----------|
| Desktop framework | Tauri v2 | Cross-platform, small binary, Rust backend |
| Frontend | Svelte 5 | Smallest bundle, cleanest syntax |
| Package manager | Bun | Faster than npm/pnpm |
| State management | AtomicBool (Rust) | Thread-safe recording state |

---

## Session 2 - December 1, 2025

### What Was Built

#### Phase 2: Screen Share Protection
- Added `set_content_protected(true)` for macOS
- Window won't appear in screen recordings/shares
- Added `set_screen_share_protection` command to toggle from frontend

#### Phase 3: Audio Capture System
Created `/app/src-tauri/src/audio.rs` with:

**AudioSample struct:**
```rust
pub struct AudioSample {
    pub data: Vec<f32>,
    pub sample_rate: u32,
    pub channels: u16,
    pub source: AudioSource,  // Microphone or SystemAudio
    pub timestamp_ms: u64,
}
```

**AudioCapture manager:**
- Manages dual audio streams (mic + system)
- Thread-safe with `Arc<AtomicBool>` for capture state
- Uses `tokio::sync::mpsc` channels for audio streaming

**Platform-specific implementations:**

| Platform | Microphone | System Audio |
|----------|------------|--------------|
| macOS | cpal (CoreAudio) | ScreenCaptureKit |
| Windows | cpal (WASAPI) | WASAPI Loopback |
| Linux | cpal | Not supported yet |

**macOS ScreenCaptureKit config:**
```rust
config.captures_audio = true;
config.excludes_current_process_audio = true;  // Don't capture our own audio
config.sample_rate = 48000;
config.channel_count = 2;
```

**Windows WASAPI Loopback:**
- Uses `AUDCLNT_STREAMFLAGS_LOOPBACK` to capture system audio
- Gets default render endpoint for loopback
- Converts audio data to f32 format

#### Frontend Updates
- Added event listener for `audio-sample` events
- Display real-time audio chunk counters
- Separate counters for Mic (You) vs System (Guests)

### Files Created/Modified
- `/app/src-tauri/Cargo.toml` - Added cpal, screencapturekit, windows crates
- `/app/src-tauri/src/audio.rs` - New audio capture module
- `/app/src-tauri/src/lib.rs` - Integrated audio capture with recording commands
- `/app/src/routes/+page.svelte` - Audio stats UI

### Dependencies Added
```toml
cpal = "0.15"  # Cross-platform microphone

# macOS
screencapturekit = "0.2"
screencapturekit-sys = "0.2"

# Windows
windows = { version = "0.58", features = ["Win32_Media_Audio", ...] }
```

---

## Session 2 (continued) - Speech Processing Pipeline

### What Was Built

#### Phase 4: Transcription Service (Sherpa-ONNX WASM)
Created `/app/src/lib/transcription.ts`:

**Features:**
- Streaming ASR using Sherpa-ONNX WASM
- Dual streams for microphone (You) and system audio (Guests)
- Auto-resampling to 16kHz
- Endpoint detection for sentence boundaries

**Config:**
```typescript
{
  sampleRate: 16000,
  featureDim: 80,
  decodingMethod: 'greedy_search',
  enableEndpoint: 1,
  rule1MinTrailingSilence: 2.4,  // End after 2.4s silence mid-sentence
  rule2MinTrailingSilence: 1.2,  // End after 1.2s silence at sentence end
}
```

**Model:** sherpa-onnx-streaming-zipformer-en-20M (~20MB)

#### Phase 5: Voice Activity Detection (Silero VAD)
Created `/app/src/lib/vad.ts`:

**Features:**
- Ultra-fast speech detection (<1ms per chunk)
- 512 sample windows (~32ms at 16kHz)
- LSTM state tracking for context
- 0.5 probability threshold

**Config:**
```typescript
{
  SAMPLE_RATE: 16000,
  WINDOW_SIZE: 512,
  THRESHOLD: 0.5,
}
```

**Model:** silero_vad.onnx (~2MB)

#### Phase 6: Smart Turn Detection
Created `/app/src/lib/smart-turn.ts`:

**Signals analyzed:**
1. **Silence duration** - VAD-based, configurable thresholds
2. **Sentence completion** - Detects `.!?` endings
3. **Turn phrases** - "what do you think?", "over to you", etc.
4. **Question detection** - Reduces confidence (expects response)

**Config:**
```typescript
{
  minSilenceForTurn: 700,      // Min silence to consider turn end (ms)
  maxSilenceBeforeCutoff: 2000, // Force turn end (ms)
  sentenceCompleteBonus: 0.3,   // Confidence boost for complete sentences
  turnPhraseBonus: 0.4,         // Confidence boost for turn phrases
  questionPenalty: 0.2,         // Penalty for questions
}
```

#### Phase 7: Audio Pipeline
Created `/app/src/lib/audio-pipeline.ts`:

**Coordinates:**
- VAD â†’ Transcription â†’ Turn Detection
- Manages transcript segments per speaker
- Emits events for UI consumption

**Events:**
```typescript
interface PipelineEvents {
  onTranscript: (segment: TranscriptSegment) => void;
  onVAD: (result: VADResult) => void;
  onTurnEnd: (result: TurnResult) => void;
  onError: (error: Error) => void;
}
```

### Files Created
- `/app/src/lib/transcription.ts` - Sherpa-ONNX service
- `/app/src/lib/vad.ts` - Silero VAD service
- `/app/src/lib/smart-turn.ts` - Turn detection
- `/app/src/lib/audio-pipeline.ts` - Pipeline coordinator
- `/app/scripts/download-models.sh` - Model downloader

### Automatic Model Download (First Boot)

Models are automatically downloaded on first app launch - no manual setup required.

**Implementation:**
- `/app/src-tauri/src/models.rs` - Rust model manager
- `/app/src/lib/components/ModelSetup.svelte` - UI with progress bar

**Flow:**
1. App checks if models exist in `~/.local/share/second-brain/models/`
2. If missing, shows download UI with progress bar
3. Downloads each model with progress events via Tauri
4. Extracts archives (tar.bz2) automatically
5. Continues to main app when complete

**Models Downloaded:**
| Model | Size | Purpose |
|-------|------|---------|
| silero_vad.onnx | ~2MB | Voice Activity Detection |
| sherpa-zipformer-en-20M | ~20MB | Streaming ASR |

**Rust Dependencies Added:**
```toml
reqwest = { version = "0.12", features = ["stream"] }
futures-util = "0.3"
flate2 = "1.0"
tar = "0.4"
dirs = "5.0"
```

**Events Emitted:**
```rust
struct DownloadProgress {
    model_id: String,
    model_name: String,
    downloaded_bytes: u64,
    total_bytes: u64,
    progress_percent: f32,
    status: String,  // "downloading", "extracting", "complete"
}
```

---

## Session 3 - Live Transcription UI

### What Was Built

#### Phase 8: Audio Pipeline Integration
- Rust backend now sends actual audio samples (not just metadata)
- Buffered sending (~100ms chunks) to reduce event frequency
- Both `audio-sample` (lightweight status) and `audio-data` (actual samples) events

#### Phase 9: Live Transcript UI
Created `/app/src/lib/components/TranscriptView.svelte`:

**Features:**
- Real-time transcript display with speaker labels
- "You" (microphone) vs "Guest 1, 2..." (system audio)
- Live typing indicator for current speech
- Color-coded segments per speaker
- Auto-scroll to latest text
- Clear transcript button

**UI States:**
- Empty state with mic icon
- Live segments (pulsing animation)
- Completed segments with timestamps
- Error banner if pipeline fails

### Files Modified
- `/app/src-tauri/src/lib.rs` - Added audio data streaming
- `/app/src/routes/+page.svelte` - Integrated pipeline and transcript
- `/app/src/lib/components/TranscriptView.svelte` - New component

### Data Flow
```
Rust Audio Capture
       â†“
   audio-data event (100ms chunks)
       â†“
   Frontend receives Float32Array
       â†“
   AudioPipeline processes:
   â”œâ”€â”€ VAD (speech detection)
   â”œâ”€â”€ Transcription (Sherpa-ONNX)
   â””â”€â”€ Smart Turn (endpoint detection)
       â†“
   TranscriptView displays results
```

---

## Session 4 - Bug Fixes & Compilation Cleanup

### What Was Fixed

#### Issue 1: ScreenCaptureKit API Incompatibility
The `screencapturekit` crate API was different than expected:
- `SCStreamOutputType` didn't implement `PartialEq`
- `CMSampleBuffer::get_audio_buffer_list()` method didn't exist

**Solution:** Replaced ScreenCaptureKit with cpal-based loopback audio capture. This uses virtual audio devices (BlackHole, Loopback, Soundflower) for system audio capture on macOS.

#### Issue 2: Arc<AtomicBool> Move Error
```rust
error[E0382]: borrow of moved value: `is_capturing`
```
The `is_capturing` Arc was moved into the closure and then borrowed again.

**Solution:** Clone the Arc before moving into closure:
```rust
let is_capturing_for_callback = is_capturing.clone();
let stream = device.build_input_stream(
    &config.into(),
    move |data: &[f32], _| {
        if !is_capturing_for_callback.load(Ordering::SeqCst) {  // Use clone
            return;
        }
        // ...
    },
    // ...
);
while is_capturing.load(Ordering::SeqCst) {  // Original still works
    // ...
}
```

#### Issue 3: Unused Import Warnings
- Removed unused `Arc` import from `lib.rs` (already imported via `std::sync::atomic`)
- Removed unused `flate2::read::GzDecoder` import from `models.rs`

#### Issue 4: Deprecated Tauri API
```rust
warning: use of deprecated method `TrayIconBuilder::menu_on_left_click`
```
**Solution:** Changed to `show_menu_on_left_click(false)`

#### Issue 5: onnxruntime-web Not Installed
```
Cannot find module 'onnxruntime-web'
```
**Solution:** `bun add onnxruntime-web`

#### Issue 6: Unused ts-expect-error Directive
In `vite.config.js`, the `@ts-expect-error` comment was no longer needed.

**Solution:** Removed the unnecessary comment.

#### Issue 7: Smart Turn VAD Integration Bug
The VAD results were not being passed to SmartTurn service. The callback was empty:
```typescript
sileroVAD.onVAD((result) => {
  // We'll process both mic and system VAD separately
});
```

**Solution:**
1. Updated `vad.ts` to return `VADResult | null` from `processAudio()`
2. Updated `audio-pipeline.ts` to capture VAD results and pass them to SmartTurn with source info:
```typescript
const vadResult = await sileroVAD.processAudio(samples, sampleRate);
if (vadResult) {
  smartTurn.processVAD(vadResult, 'microphone');  // or 'system'
}
```

Now Smart Turn can properly track silence duration per-speaker for turn detection.

### Files Modified
- `/app/src-tauri/src/audio.rs` - Rewrote macOS system audio capture to use cpal loopback
- `/app/src-tauri/src/lib.rs` - Fixed Arc import, updated deprecated API
- `/app/src-tauri/src/models.rs` - Removed unused import
- `/app/src-tauri/Cargo.toml` - Removed screencapturekit dependencies
- `/app/vite.config.js` - Removed unused ts-expect-error
- `/app/src/lib/vad.ts` - Updated `processAudio()` to return VADResult
- `/app/src/lib/audio-pipeline.ts` - Fixed VAD â†’ SmartTurn integration

### Current Audio Architecture

| Platform | Microphone | System Audio |
|----------|------------|--------------|
| macOS | cpal (CoreAudio) | cpal loopback (requires BlackHole/Loopback) |
| Windows | cpal (WASAPI) | WASAPI Loopback (native) |
| Linux | cpal | Not supported |

**macOS System Audio Note:**
Users need to install [BlackHole](https://existential.audio/blackhole/) or similar virtual audio device and configure their system to route audio through it. The app automatically detects these devices by name.

### Compilation Status
- **TypeScript:** 0 errors, 0 warnings
- **Rust:** 0 errors, 0 warnings

---

## Session 5 - Native ASR Migration (WASM â†’ Rust)

### What Was Built

#### Phase 10: Migrated ASR from Browser WASM to Native Rust

**Why the change:**
- Browser WASM had issues loading ONNX models (404 errors, backend compatibility)
- Native Rust provides better performance and reliability
- User requested: "wait literally exists bro https://github.com/thewh1teagle/sherpa-rs"

**New Architecture:**
```
Audio Capture (Rust/cpal)
       â†“
   Audio samples via tokio channel
       â†“
   ASR Processing Thread (Rust)
   â”œâ”€â”€ Silero VAD (speech detection)
   â””â”€â”€ ZipFormer (transcription)
       â†“
   "transcription" event to frontend
       â†“
   TranscriptView displays results
```

#### Rust Backend Changes

**New File: `/app/src-tauri/src/asr.rs`**
```rust
pub struct AsrEngine {
    config: AsrConfig,
    mic_vad: Option<SileroVad>,      // Separate VAD for microphone
    system_vad: Option<SileroVad>,   // Separate VAD for system audio
    recognizer: Option<ZipFormer>,
}
```

**Key Methods:**
- `initialize()` - Loads VAD and ASR models
- `process_microphone()` - VAD + transcription for mic audio
- `process_system()` - VAD + transcription for system audio

**Processing Flow:**
1. Audio fed to VAD via `accept_waveform()`
2. VAD detects speech segments via `is_empty()`, `front()`, `pop()`
3. Speech segments passed to `ZipFormer::decode()`
4. Transcription emitted as Tauri event

**Cargo.toml Addition:**
```toml
# Speech recognition - sherpa-onnx Rust bindings
sherpa-rs = { version = "0.6", features = ["download-binaries"] }
```

**lib.rs Changes:**
- Added `initialize_asr` command
- Added `asr` module
- Modified `start_recording` to spawn ASR processing thread
- Created dedicated Tokio runtime in thread (Tauri 2 doesn't provide one for sync commands)

#### Frontend Changes

**Simplified `/app/src/lib/transcription.ts`:**
- Now just listens for Rust `transcription` events
- No more WASM loading or processing
- Kept API compatible for minimal changes elsewhere

**Simplified `/app/src/lib/audio-pipeline.ts`:**
- Removed frontend VAD processing
- Removed `processMicrophoneAudio()` / `processSystemAudio()` methods
- Just initializes transcription listener and handles events

**Removed `/app/src/lib/vad.ts`:**
- VAD now handled in Rust backend

**Updated `/app/src/lib/smart-turn.ts`:**
- Removed VAD dependency
- Turn detection now based on transcription timing/content only

**Updated `/app/src/routes/+page.svelte`:**
- Added `initialize_asr` call on app init
- Removed `audio-data` event listener (no longer needed)

**Removed Dependencies:**
```json
// Removed from package.json:
"onnxruntime-web": "^1.23.2",
"sherpa-onnx": "^1.12.18"
```

**Removed Files:**
- `/app/static/ort-wasm-simd-threaded.*` (WASM runtime files)

#### Model Changes

**Silero VAD:**
- Changed URL from `snakers4/silero-vad` to `k2-fsa/sherpa-onnx/releases`
- sherpa-rs requires the sherpa-onnx compatible version

**ZipFormer ASR:**
- Changed from **streaming** to **offline** model
- sherpa-rs `ZipFormer` struct expects non-streaming format
- Model: `sherpa-onnx-zipformer-gigaspeech-2023-12-12` (~70MB compressed)
- File naming: `epoch-30` instead of `epoch-99`

**Models Downloaded:**
| Model | Size | Source | Purpose |
|-------|------|--------|---------|
| silero_vad.onnx | ~2MB | k2-fsa/sherpa-onnx | Voice Activity Detection |
| encoder-epoch-30-avg-1.onnx | ~261MB | sherpa-onnx | ASR Encoder |
| decoder-epoch-30-avg-1.onnx | ~2MB | sherpa-onnx | ASR Decoder |
| joiner-epoch-30-avg-1.onnx | ~1MB | sherpa-onnx | ASR Joiner |
| tokens.txt | ~5KB | sherpa-onnx | Vocabulary |
| bpe.model | ~245KB | sherpa-onnx | BPE tokenizer |

### Errors & Solutions

#### Error 1: Protobuf Parsing Failed
```
Ort::Exception: Failed to load model because protobuf parsing failed.
```
**Cause:** Wrong Silero VAD model version. Downloaded from `snakers4/silero-vad` but sherpa-rs expects the version from `k2-fsa/sherpa-onnx/releases`.

**Solution:** Updated model URL to: `https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/silero_vad.onnx`

#### Error 2: No Tokio Reactor Running
```
thread 'main' panicked at src/lib.rs:80:5:
there is no reactor running, must be called from the context of a Tokio 1.x runtime
```
**Cause:** Used `tokio::spawn()` in a Tauri command, but Tauri 2 doesn't provide a Tokio runtime for sync commands.

**Solution:** Create dedicated Tokio runtime in spawned thread:
```rust
std::thread::spawn(move || {
    let rt = tokio::runtime::Builder::new_current_thread()
        .enable_all()
        .build()
        .expect("Failed to create tokio runtime");

    rt.block_on(async move {
        while let Some(sample) = tokio_rx.recv().await {
            // Process audio...
        }
    });
});
```

#### Error 3: Wrong Model File Names
**Cause:** Downloaded streaming model has `epoch-99` files, but we needed offline model with `epoch-30` files.

**Solution:** Updated `models.rs` to download offline model and use correct file names.

### Files Created/Modified

**Created:**
- `/app/src-tauri/src/asr.rs` - Native ASR module

**Modified:**
- `/app/src-tauri/Cargo.toml` - Added sherpa-rs
- `/app/src-tauri/src/lib.rs` - Integrated ASR, fixed Tokio runtime
- `/app/src-tauri/src/models.rs` - Updated model URLs and file names
- `/app/src/lib/transcription.ts` - Simplified to event listener
- `/app/src/lib/audio-pipeline.ts` - Removed frontend processing
- `/app/src/lib/smart-turn.ts` - Removed VAD dependency
- `/app/src/routes/+page.svelte` - Added ASR init, removed audio-data listener
- `/app/package.json` - Removed WASM dependencies

**Deleted:**
- `/app/src/lib/vad.ts` - VAD now in Rust
- `/app/static/ort-wasm-simd-threaded.*` - WASM runtime files

### Current Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Tauri App                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Frontend (Svelte)  â”‚          Backend (Rust)              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                      â”‚                                       â”‚
â”‚  TranscriptView â†â”€â”€â”€â”€â”¼â”€â”€â”€â”€ "transcription" event            â”‚
â”‚       â†‘              â”‚            â†‘                          â”‚
â”‚  audio-pipeline.ts   â”‚       ASR Thread                      â”‚
â”‚       â†‘              â”‚       â”œâ”€â”€ SileroVad (mic)            â”‚
â”‚  transcription.ts    â”‚       â”œâ”€â”€ SileroVad (system)         â”‚
â”‚  (event listener)    â”‚       â””â”€â”€ ZipFormer                  â”‚
â”‚                      â”‚            â†‘                          â”‚
â”‚                      â”‚       Audio Thread                    â”‚
â”‚                      â”‚       (tokio runtime)                 â”‚
â”‚                      â”‚            â†‘                          â”‚
â”‚                      â”‚       Audio Capture                   â”‚
â”‚                      â”‚       â”œâ”€â”€ cpal (microphone)          â”‚
â”‚                      â”‚       â””â”€â”€ cpal loopback (system)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Benefits of Native ASR

1. **Performance** - No WASM overhead, native CPU/GPU acceleration
2. **Reliability** - No browser compatibility issues
3. **Simplicity** - Single codebase for audio + ASR
4. **Lower Latency** - Direct memory access, no serialization

### Compilation Status
- **TypeScript:** 0 errors, 0 warnings
- **Rust:** 0 errors, 0 warnings

---

## Session 6 - Knowledge Base Integration (In Progress)

### What's Being Built

#### Phase 11: Knowledge Base System
Adding contextual intelligence with:
- **Entity Extraction** - GLiNER model for NER (people, topics, actions, decisions)
- **Embeddings** - EmbeddingGemma-300M for semantic search
- **SurrealDB** - Embedded graph + vector + full-text search database
- **LLM Agent** - rig.rs for intelligent queries (using self-hosted vLLM endpoint)

### Architecture

```
Transcription (existing)
       â†“
Entity Extraction (GLiNER)
â”œâ”€â”€ People mentioned
â”œâ”€â”€ Topics/Projects
â”œâ”€â”€ Action items
â””â”€â”€ Decisions
       â†“
Embedding Engine (EmbeddingGemma-300M)
       â†“
Knowledge Base (SurrealDB)
â”œâ”€â”€ Meetings table
â”œâ”€â”€ Transcript segments (with embeddings)
â”œâ”€â”€ Action items
â”œâ”€â”€ Decisions
â”œâ”€â”€ People (nodes)
â”œâ”€â”€ Topics (nodes)
â””â”€â”€ Relations (graph edges)
       â†“
LLM Agent (rig.rs)
â”œâ”€â”€ Search past meetings
â”œâ”€â”€ Track action items
â””â”€â”€ Generate insights
```

### New Files Created

**`/app/src-tauri/src/entities.rs`**
- GLiNER-based NER using gline-rs crate
- Entity types: person, organization, project, action_item, deadline, decision, topic, metric, product, location
- Batch processing support

**`/app/src-tauri/src/embeddings.rs`**
- EmbeddingGemma-300M via ONNX Runtime
- 768-dim embeddings
- Batch embedding support
- Cosine similarity utilities

**`/app/src-tauri/src/knowledge_base.rs`**
- SurrealDB embedded database (RocksDB backend)
- Meeting/segment/action/decision storage
- Vector similarity search
- Graph relations (personâ†”meeting, topicâ†”meeting)
- Full-text search

### New Dependencies

```toml
# Entity Extraction
gline-rs = "1"
orp = "0.9"

# Embeddings
ort = { version = "2.0.0-rc.9", features = ["download-binaries"] }
tokenizers = { version = "0.21", default-features = false, features = ["onig"] }
ndarray = "0.16"

# Database
surrealdb = { version = "2.1", features = ["kv-rocksdb"] }

# LLM Agent
rig-core = "0.8"
```

### New Models (Auto-Downloaded)

| Model | Size | Purpose |
|-------|------|---------|
| GLiNER small v2.1 | ~50MB | Named Entity Recognition |
| GLiNER tokenizer | ~2MB | GLiNER tokenizer |
| EmbeddingGemma Q4 | ~200MB | Text embeddings (768-dim) |
| EmbeddingGemma tokenizer | ~5MB | Embedding tokenizer |

### New Tauri Commands

```rust
// Initialization
initialize_entities()      // Load GLiNER model
initialize_embeddings()    // Load EmbeddingGemma
initialize_knowledge_base() // Open SurrealDB

// Meeting management
start_meeting(title, participants) -> meeting_id
end_meeting(summary)
add_transcript_segment(speaker, text, start_ms, end_ms)

// Queries
search_knowledge(query, limit) -> SearchResult[]
get_action_items() -> ActionItem[]
get_decisions(limit) -> Decision[]
extract_entities(text) -> ExtractionResult
```

### Database Schema (SurrealDB)

```sql
-- Core tables
meeting, segment, action_item, decision, person, topic

-- Graph relations
mentioned_in   (person -> meeting)
participated_in (person -> meeting)
discussed_in   (topic -> meeting)
assigned_to    (action_item -> person)

-- Vector index on segment.embedding for similarity search
```

### LLM Configuration (Next Step)

```rust
let client = openai::Client::from_url(
    "https://lmstudio.subh-dev.xyz/llm/v1",
    "dummy-key"
);

let agent = client
    .agent("openai/gpt-oss-20b")
    .preamble("You are a meeting assistant...")
    .tool(SearchTranscriptsTool { kb: kb.clone() })
    .build();
```

### Current Status
- âœ… gline-rs dependency added
- âœ… entities.rs module created
- âœ… GLiNER model auto-download configured
- âœ… embeddings.rs module created
- âœ… EmbeddingGemma model auto-download configured
- âœ… knowledge_base.rs with SurrealDB created
- âœ… Tauri commands for KB operations
- âœ… Fixed compilation errors (lifetime issues with SurrealDB .bind())
- âœ… LLM agent with rig.rs completed
- â³ Frontend integration

### LLM Agent Implementation

**`/app/src-tauri/src/llm_agent.rs`**

Created LLM-powered meeting assistant using rig.rs:

```rust
// Custom error type implementing std::error::Error
pub struct ToolError(String);

// Tool for searching past transcripts
pub struct SearchTranscriptsTool {
    pub kb: Arc<RwLock<Option<KnowledgeBase>>>,
}

impl Tool for SearchTranscriptsTool {
    const NAME: &'static str = "search_transcripts";
    type Args = SearchTranscriptsArgs;
    type Output = String;
    type Error = ToolError;

    async fn definition(&self, _prompt: String) -> ToolDefinition { ... }
    async fn call(&self, args: Self::Args) -> Result<Self::Output, Self::Error> { ... }
}

// Tool for getting action items
pub struct GetActionItemsTool { ... }

// Main assistant
#[derive(Clone)]
pub struct MeetingAssistant {
    client: openai::Client,
    model: String,
}

impl MeetingAssistant {
    pub fn new(api_url: &str, model: &str) -> Self { ... }
    pub async fn ask(&self, question: &str, kb: Arc<...>) -> Result<String, String> { ... }
    pub async fn summarize_meeting(&self, segments: &[String]) -> Result<String, String> { ... }
    pub async fn suggest_questions(&self, topic: &str, kb: Arc<...>) -> Result<Vec<String>, String> { ... }
}
```

**Key Changes:**
- Used `rig::completion::ToolDefinition` (not `rig::tool::ToolDefinition` which is private)
- Created `ToolError` struct implementing `std::error::Error` (required by rig's Tool trait)
- Made `MeetingAssistant` derive `Clone` to avoid MutexGuard lifetime issues across await points
- Fixed `format!()` returns `String`, must pass owned value not `&String` to `agent.prompt()`

**New Tauri Commands:**
```rust
initialize_llm(url, model_name)  // Initialize LLM client
ask_assistant(question)           // Ask question with tool use
summarize_meeting(segments)       // Generate meeting summary
suggest_questions(current_topic)  // Get suggested questions
```

### Errors & Solutions

#### Error 1: ToolDefinition is Private
```
error[E0603]: struct `ToolDefinition` is private
help: consider importing this struct instead: `rig::completion::ToolDefinition`
```
**Solution:** Changed `rig::tool::ToolDefinition` to `rig::completion::ToolDefinition`

#### Error 2: String Doesn't Implement StdError
```
error[E0277]: the trait bound `std::string::String: StdError` is not satisfied
```
**Solution:** Created custom `ToolError` struct:
```rust
#[derive(Debug)]
pub struct ToolError(String);

impl std::fmt::Display for ToolError { ... }
impl std::error::Error for ToolError {}
impl From<String> for ToolError { ... }
```

#### Error 3: MutexGuard Lifetime Across Await
```
error: future cannot be sent between threads safely
  = help: within `impl Future<...>`, the trait `Send` is not implemented for `MutexGuard<...>`
```
**Solution:** Made `MeetingAssistant` derive `Clone`, then clone before await:
```rust
let assistant = {
    let guard = state.llm_assistant.lock().unwrap();
    guard.as_ref().ok_or("...")?.clone()
};
assistant.ask(&question, kb).await  // Guard released before await
```

#### Error 4: &String Not Into<Message>
```
error[E0277]: the trait bound `rig::completion::Message: From<&String>` is not satisfied
```
**Solution:** Pass owned `String` instead of reference:
```rust
// Before
agent.prompt(&format!("..."))

// After
let prompt = format!("...");
agent.prompt(prompt)
```

### Recording Pipeline Integration

Modified the ASR processing thread in `lib.rs` to auto-save final transcripts to the knowledge base:

```rust
// In ASR processing thread
if transcription.is_final && !transcription.text.trim().is_empty() {
    let meeting_id = state.current_meeting_id.lock().unwrap().clone();
    if let Some(meeting_id) = meeting_id {
        let kb = state.knowledge_base.clone();
        let speaker = if source == "microphone" { "You" } else { "Guest" };

        rt.block_on(async {
            let kb_guard = kb.read().await;
            if let Some(ref kb) = *kb_guard {
                kb.add_segment(&meeting_id, &speaker, &text, timestamp, timestamp + 1000).await;
            }
        });
    }
}
```

**Flow:**
1. User calls `start_meeting(title, participants)` â†’ creates meeting in KB, stores ID
2. Recording starts â†’ ASR processes audio
3. When `is_final` transcript arrives:
   - Check if meeting is active (has ID)
   - Save segment to KB with speaker/text/timestamp
   - KB auto-extracts entities and creates graph relations
4. User calls `end_meeting(summary)` â†’ closes meeting in KB

### Frontend UI for Knowledge Search

Created `/app/src/lib/components/KnowledgeSearch.svelte`:

**Features:**
- Meeting management (start/end with title)
- Three tabs: Search, Ask AI, Actions
- Vector similarity search for past transcripts
- LLM-powered Q&A with tool use
- Action item tracking

**UI Components:**
```svelte
<!-- Meeting Controls -->
<input placeholder="Meeting title..." />
<button>Start Meeting</button>

<!-- Tabs -->
<button class="tab">Search</button>
<button class="tab">Ask AI</button>
<button class="tab">Actions</button>

<!-- Search Tab -->
<input placeholder="Search past meetings..." />
<div class="results">...</div>

<!-- Ask AI Tab -->
<input placeholder="Ask about past meetings..." />
<div class="response">...</div>

<!-- Actions Tab -->
<div class="action-list">...</div>
```

**Auto-initialization:**
- On component mount, initializes:
  - Entity engine (GLiNER)
  - Embedding engine (EmbeddingGemma)
  - Knowledge base (SurrealDB)
  - LLM assistant (rig.rs with self-hosted endpoint)

**Integration with Main Page:**
- Added import for KnowledgeSearch component
- Added Knowledge Base section below transcript view
- Updated info panel with KB instructions

### Compilation Status
- **TypeScript:** 0 errors, 0 warnings
- **Rust:** 0 errors, 6 warnings (unused code)

---

## Session 7 - Second Brain UI Redesign + User Store

### What Was Built

#### Phase 12: Complete UI Redesign

Replaced simple search UI with full "Second Brain" app interface.

**New File: `/app/src/lib/components/SecondBrain.svelte`**

A complete app-like interface with sidebar navigation and multiple views:

**Sidebar Navigation:**
- ğŸ  Home - Dashboard with agenda, insights, recent meetings
- ğŸ“… Meetings - Full meeting list with details
- ğŸ“ Notes - Personal notes with tags
- ğŸ’¡ Insights - AI patterns, action items
- ğŸ”Œ Integrations - Connected tools

**Home Dashboard Features:**
- Greeting with current date
- Global search bar ("Search your brain...")
- Active meeting banner (red, pulsing when recording)
- Start meeting card (enter title â†’ start recording)
- Today's Agenda - Upcoming meetings, action items, follow-ups
- AI Insights - Patterns, commitments, connections
- Recent Meetings - With participants, topics, action count
- Ask Your Brain - Natural language Q&A

**Design System:**
```css
/* Dark mode (default) */
--bg-primary: #0f0f10;
--bg-sidebar: #18181b;
--accent-color: #818cf8;

/* Status colors */
--priority-high: #ef4444;
--priority-medium: #f59e0b;
--priority-low: #22c55e;
```

**Updated `/app/src/routes/+page.svelte`:**
- Simplified to just load SecondBrain component
- Removed old transcript view from main page

#### Phase 13: User Store (SQLite with rusqlite)

Added persistent storage for user-level data separate from SurrealDB knowledge graph.

**New Dependency:**
```toml
rusqlite = { version = "0.32", features = ["bundled"] }
```

**New File: `/app/src-tauri/src/user_store.rs`**

SQLite database at `~/.local/share/second-brain/user_store.db`

**Tables:**
| Table | Purpose |
|-------|---------|
| `settings` | User preferences (singleton) |
| `notes` | Quick notes with tags |
| `integrations` | Connected tools (OAuth tokens) |
| `saved_searches` | Saved search queries |
| `app_state` | Key-value for misc state |

**Data Structures:**
```rust
pub struct UserSettings {
    pub theme: String,           // "dark", "light", "system"
    pub llm_url: String,         // LLM API endpoint
    pub llm_model: String,       // Model name
    pub auto_record: bool,
    pub notifications_enabled: bool,
    pub language: String,
}

pub struct Note {
    pub id: i64,
    pub content: String,
    pub tags: Vec<String>,
    pub pinned: bool,
    pub created_at: String,
    pub updated_at: String,
}

pub struct Integration {
    pub id: String,              // "google_calendar", "slack"
    pub name: String,
    pub status: String,          // "connected", "disconnected"
    pub access_token: Option<String>,
    pub refresh_token: Option<String>,
}
```

**New Tauri Commands:**

Settings:
- `initialize_user_store` - Must call on app start
- `get_user_settings` â†’ `UserSettings`
- `update_user_settings(settings)`
- `set_user_setting(key, value)`

Notes:
- `create_note(content, tags)` â†’ `Note`
- `get_notes(limit?)` â†’ `Vec<Note>`
- `update_note(id, content, tags)` â†’ `Note`
- `toggle_note_pin(id)` â†’ `Note`
- `delete_note(id)`

Integrations:
- `get_integrations()` â†’ `Vec<Integration>`
- `upsert_integration(integration)`
- `disconnect_integration(id)`

Saved Searches:
- `save_search(query, name)` â†’ `SavedSearch`
- `get_saved_searches()` â†’ `Vec<SavedSearch>`
- `delete_saved_search(id)`

App State (Key-Value):
- `get_app_state(key)` â†’ `Option<String>`
- `set_app_state(key, value)`

#### Phase 14: Design Guide

Created `/DESIGN_GUIDE.md` with:
- Complete design system (colors, typography, spacing)
- Component patterns (cards, buttons, inputs, badges)
- App structure and navigation
- State management patterns
- Instructions for adding new features
- How to connect mock data to real backend

### Database Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Second Brain Data                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  SurrealDB (Knowledge)      â”‚  SQLite (User Store)          â”‚
â”‚  ~/.../knowledge.db         â”‚  ~/.../user_store.db          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ Meetings                 â”‚  â€¢ Settings (preferences)     â”‚
â”‚  â€¢ Transcript segments      â”‚  â€¢ Notes (quick capture)      â”‚
â”‚  â€¢ Embeddings (vectors)     â”‚  â€¢ Integrations (OAuth)       â”‚
â”‚  â€¢ Entities (NER)           â”‚  â€¢ Saved searches             â”‚
â”‚  â€¢ Action items             â”‚  â€¢ App state (k-v)            â”‚
â”‚  â€¢ Decisions                â”‚                               â”‚
â”‚  â€¢ Graph relations          â”‚                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Files Created/Modified

**Created:**
- `/app/src/lib/components/SecondBrain.svelte` - Main app UI
- `/app/src-tauri/src/user_store.rs` - SQLite user store
- `/DESIGN_GUIDE.md` - Comprehensive design documentation

**Modified:**
- `/app/src/routes/+page.svelte` - Simplified to use SecondBrain
- `/app/src-tauri/Cargo.toml` - Added rusqlite
- `/app/src-tauri/src/lib.rs` - Added user_store module and 17 new commands

### Compilation Status
- **TypeScript:** 0 errors, 0 warnings
- **Rust:** 0 errors, 6 warnings (unused utility functions)

---

## Session 8 - Frontend User Store Integration

### What Was Built

#### Phase 15: Wired Frontend to User Store

Connected the SecondBrain UI to the SQLite user store backend.

**Updated `/app/src/routes/+page.svelte`:**
- Added `initialize_user_store` call in `initializeApp()` on app start
- User store now initializes before ASR

**Updated `/app/src/lib/components/SecondBrain.svelte`:**

**TypeScript Interfaces Added:**
```typescript
interface Note {
  id: number;
  content: string;
  tags: string[];
  pinned: boolean;
  created_at: string;
  updated_at: string;
}

interface Integration {
  id: string;
  name: string;
  status: string;
  access_token: string | null;
  // ...
}

interface UserSettings {
  id: number;
  theme: string;
  llm_url: string;
  llm_model: string;
  auto_record: boolean;
  notifications_enabled: boolean;
  language: string;
  // ...
}
```

**New Reactive State:**
```typescript
let notes = $state<Note[]>([]);
let integrations = $state<Integration[]>([]);
let userSettings = $state<UserSettings | null>(null);
```

**Data Loading Functions:**
- `loadNotes()` - Fetches notes from `get_notes` command
- `loadIntegrations()` - Fetches from `get_integrations` command
- `loadSettings()` - Fetches from `get_user_settings` command
- All called via `onMount()`

**Note Management Functions:**
- `createNote()` - Creates note with tags (comma-separated input)
- `createQuickNote()` - Quick note from sidebar (no tags)
- `togglePin(noteId)` - Pin/unpin notes
- `deleteNote(noteId)` - Delete with confirmation

**Integration Management:**
- `toggleIntegration(toolId, toolName)` - Connect/disconnect
- Placeholder for future OAuth flows
- Currently just saves connection status to SQLite

**UI Helpers:**
- `formatRelativeTime(isoDate)` - "Just now", "2 hours ago", etc.
- Computed `connectedTools` - Merges available integrations with saved status

**Notes View Updates:**
- Shows real notes from SQLite (pinned first)
- Note count badge in header
- Add note card with content + tags input
- Hover actions: pin/unpin, delete
- Empty state when no notes
- Proper tag display with # prefix

**Integrations View Updates:**
- Connect/Disconnect buttons now functional
- Status persists across app restarts
- Visual feedback for connected state

**Sidebar Quick Note:**
- Creates real notes via `createQuickNote()`
- Clears input on Enter or button click

### CSS Additions

```css
/* Note card styles */
.note-card.pinned { border-color: var(--accent-color); background: var(--accent-bg); }
.note-header { display: flex; justify-content: space-between; }
.note-actions { opacity: 0; transition: opacity 0.15s; }
.note-card:hover .note-actions { opacity: 1; }
.note-action-btn.delete:hover { background: rgba(239, 68, 68, 0.2); color: #ef4444; }

/* Empty state */
.empty-state { grid-column: 1 / -1; padding: 48px; text-align: center; }
.empty-icon { font-size: 3rem; opacity: 0.5; }

/* Tags input */
.add-note-tags { padding: 8px; border-radius: 6px; background: var(--bg-input); }
.tag-hint { font-size: 0.75rem; color: var(--text-secondary); }
```

### Files Modified
- `/app/src/routes/+page.svelte` - Added user store initialization
- `/app/src/lib/components/SecondBrain.svelte` - Full data integration

### Compilation Status
- **TypeScript:** 0 errors, 0 warnings
- **Rust:** 0 errors, 6 warnings (unused utility functions)

---

## Next Steps

### Immediate
1. Add live transcript display during recording
2. Replace remaining mock data (meetings, action items) with KB queries
3. Test end-to-end meeting flow
4. Implement search functionality

### Backend Complete âœ…
- [x] Native ASR with sherpa-rs
- [x] Knowledge base with SurrealDB (meetings, segments, entities)
- [x] LLM agent with rig.rs (search, ask, summarize)
- [x] User store with SQLite (settings, notes, integrations)
- [x] Entity extraction with GLiNER
- [x] Embeddings with EmbeddingGemma

### Frontend Complete âœ…
- [x] Second Brain UI redesign
- [x] Sidebar navigation (Home, Meetings, Notes, Insights, Integrations)
- [x] Dashboard with agenda, insights, recent meetings
- [x] Design guide documentation
- [x] User store integration (notes, integrations, settings)
- [x] Notes CRUD with pinning and tags
- [x] Integration connect/disconnect

### Coming Soon
- [ ] Live transcript view during meetings
- [ ] Wire meetings view to Knowledge Base
- [ ] Wire action items to Knowledge Base
- [ ] Search functionality (vector + full-text)
- [ ] Speaker diarization using embeddings
- [ ] Post-call digest generation
- [ ] Calendar integration (Google Calendar)
- [ ] Floating overlay window mode

### To Test System Audio on macOS
1. Install BlackHole: `brew install blackhole-2ch`
2. Create Multi-Output Device in Audio MIDI Setup
3. Select it as system output
4. App will automatically detect and use it

---

## Resources

- [Tauri v2 Documentation](https://v2.tauri.app)
- [Svelte 5 Runes](https://svelte.dev/docs/svelte/what-are-runes)
- [sherpa-rs GitHub](https://github.com/thewh1teagle/sherpa-rs)
- [Sherpa-ONNX GitHub](https://github.com/k2-fsa/sherpa-onnx)
- [Silero VAD](https://github.com/snakers4/silero-vad)
- [gline-rs GitHub](https://github.com/fbilhaut/gline-rs)
- [SurrealDB Docs](https://surrealdb.com/docs)
- [rig.rs Docs](https://rig.rs)
- [rusqlite Docs](https://docs.rs/rusqlite)

---

## Session 9 - UI Redesign & Tailwind Migration

### What Was Built

#### Phase 16: Tailwind CSS v4 Migration
- Upgraded project to use **Tailwind CSS v4** with `@tailwindcss/vite`
- Configured CSS-first setup in `app.css`
- Removed legacy `tailwind.config.js` and `postcss.config.js`
- Added `Inter` and `JetBrains Mono` fonts via Google Fonts

#### Phase 17: Premium UI Overhaul
- **Floating Sidebar Layout**: Implemented a "bento-box" style layout with detached, rounded panels for sidebar and main content.
- **Glassmorphism**: Extensive use of `backdrop-blur-xl` and translucent backgrounds.
- **Component Refactoring**:
  - `SecondBrain.svelte`: Converted to utility classes, added "Live" pulsing animations.
  - `TranscriptView.svelte`: Redesigned chat bubbles with speaker color coding.
  - `ModelSetup.svelte`: Created a modern, centered glass card for the first-run experience.
  - `+page.svelte`: Cleaned up global styles.

### Files Modified
- `/app/src/app.css` - New Tailwind v4 configuration
- `/app/vite.config.js` - Added Tailwind Vite plugin
- `/app/src/lib/components/SecondBrain.svelte` - Major UI refactor
- `/app/src/lib/components/TranscriptView.svelte` - UI refactor
- `/app/src/lib/components/ModelSetup.svelte` - UI refactor
- `/app/src/routes/+page.svelte` - Layout cleanup

### Compilation Status
- **TypeScript**: 0 errors
- **Rust**: 0 errors

---

## Session 10 - Tailwind CSS v4 Fix

### What Was Fixed

#### Issue: Tailwind CSS Not Loading

After the v4 migration attempt, Tailwind styles weren't being applied. Two critical issues were identified:

#### Fix 1: Vite Plugin Order
The `@tailwindcss/vite` plugin must come **before** `@sveltejs/kit/vite` in the plugins array.

**Before (broken):**
```javascript
plugins: [sveltekit(), tailwindcss()],
```

**After (working):**
```javascript
plugins: [tailwindcss(), sveltekit()],
```

#### Fix 2: Missing Layout File
SvelteKit requires a `+layout.svelte` file to import CSS globally. This file was missing.

**Created `/app/src/routes/+layout.svelte`:**
```svelte
<script>
  import "../app.css";
  let { children } = $props();
</script>

{@render children()}
```

### Files Modified
- `/app/vite.config.js` - Fixed plugin order (tailwindcss before sveltekit)

### Files Created
- `/app/src/routes/+layout.svelte` - Root layout importing app.css

### Compilation Status
- **TypeScript**: 0 errors, 1 warning (a11y on button)
- **Rust**: 0 errors
- **Build**: Successful (40KB CSS bundle generated)

---

## Session 11 - Web Crawler Agent (In Progress)

### What's Being Built

#### Phase 18: Web Crawler Agent for Knowledge Base
Adding ability to search the web, crawl URLs, and store content in the knowledge base for meeting context.

### New Dependencies Added

```toml
# Web crawler agent
spider = "2.38"
duckduckgo_search = "0.1"
text-splitter = { version = "0.28", features = ["markdown"] }
```

### New Rust Modules

#### `/app/src-tauri/src/chunker.rs`
Text chunking module using `text-splitter` crate:

```rust
pub struct DocumentChunker {
    config: ChunkerConfig,
    splitter: MarkdownSplitter<Characters>,
}

impl DocumentChunker {
    pub fn new() -> Self;
    pub fn chunk_markdown(&self, content: &str) -> Vec<Chunk>;
    pub fn chunk_with_metadata(&self, content: &str, source_url: &str, source_title: &str) -> Vec<ChunkWithMeta>;
}
```

- Uses `text-splitter` v0.28 with markdown feature
- Default chunk size: 1000 characters (~250 tokens)
- Preserves semantic boundaries (paragraphs, sentences, headings)

#### `/app/src-tauri/src/web_crawler.rs`
Web crawler for searching and fetching content:

```rust
pub struct WebCrawler {
    config: CrawlerConfig,
}

impl WebCrawler {
    pub fn new() -> Self;
    pub async fn search(&self, query: &str, limit: usize) -> Result<Vec<SearchResult>, String>;
    pub async fn crawl_url(&self, url: &str) -> Result<CrawledPage, String>;
}
```

**Features:**
- DuckDuckGo search via `duckduckgo_search` crate
- URL crawling via `reqwest` (simpler than full spider for single pages)
- HTML to markdown conversion (custom implementation)
- Extracts title from HTML `<title>` tag
- Removes script/style/nav/footer/header tags
- Converts headings, lists, links, bold, italic, code blocks

### SurrealDB Schema Additions

Added to `knowledge_base.rs`:

```sql
-- Knowledge sources (crawled URLs, documents)
DEFINE TABLE knowledge_source SCHEMAFULL;
DEFINE FIELD url ON knowledge_source TYPE string;
DEFINE FIELD title ON knowledge_source TYPE string;
DEFINE FIELD source_type ON knowledge_source TYPE string;  -- "url", "file", "search"
DEFINE FIELD raw_content ON knowledge_source TYPE string;
DEFINE FIELD tags ON knowledge_source TYPE array<string>;
DEFINE FIELD created_at ON knowledge_source TYPE int;
DEFINE FIELD last_updated ON knowledge_source TYPE int;
DEFINE INDEX idx_source_url ON knowledge_source FIELDS url UNIQUE;
DEFINE INDEX idx_source_tags ON knowledge_source FIELDS tags;

-- Knowledge chunks with embeddings
DEFINE TABLE knowledge_chunk SCHEMAFULL;
DEFINE FIELD source_id ON knowledge_chunk TYPE string;
DEFINE FIELD text ON knowledge_chunk TYPE string;
DEFINE FIELD chunk_index ON knowledge_chunk TYPE int;
DEFINE FIELD embedding ON knowledge_chunk TYPE array<float>;
DEFINE INDEX idx_chunk_source ON knowledge_chunk FIELDS source_id;

-- Meeting-knowledge links
DEFINE TABLE meeting_knowledge SCHEMAFULL;
DEFINE FIELD meeting_id ON meeting_knowledge TYPE string;
DEFINE FIELD source_id ON meeting_knowledge TYPE string;
DEFINE FIELD relevance_score ON meeting_knowledge TYPE float;
DEFINE FIELD assigned_by ON meeting_knowledge TYPE string;  -- "user" or "auto"
DEFINE INDEX idx_mk_meeting ON meeting_knowledge FIELDS meeting_id;
DEFINE INDEX idx_mk_source ON meeting_knowledge FIELDS source_id;
```

### New Knowledge Base Methods

```rust
// Add a knowledge source and auto-chunk it
pub async fn add_knowledge_source(&self, url: &str, title: &str, content: &str, source_type: &str, tags: Vec<String>) -> Result<String, String>;

// Get sources with optional tag filtering
pub async fn get_knowledge_sources(&self, tags: Option<Vec<String>>) -> Result<Vec<KnowledgeSource>, String>;

// Get single source by ID
pub async fn get_knowledge_source(&self, source_id: &str) -> Result<Option<KnowledgeSource>, String>;

// Delete source and its chunks
pub async fn delete_knowledge_source(&self, source_id: &str) -> Result<(), String>;

// Update source tags
pub async fn update_source_tags(&self, source_id: &str, tags: Vec<String>) -> Result<(), String>;

// Vector search across knowledge chunks
pub async fn search_knowledge(&self, query: &str, limit: usize, tags: Option<Vec<String>>) -> Result<Vec<KnowledgeSearchResult>, String>;

// Link source to meeting
pub async fn link_knowledge_to_meeting(&self, meeting_id: &str, source_id: &str, assigned_by: &str) -> Result<(), String>;

// Get sources linked to meeting
pub async fn get_meeting_knowledge(&self, meeting_id: &str) -> Result<Vec<KnowledgeSource>, String>;
```

### New Tauri Commands

```rust
// Web search/crawl
search_web(query, limit) -> Vec<WebSearchResult>
crawl_url(url) -> CrawledPage
crawl_and_store(url, tags) -> String  // Returns source_id

// Knowledge base management
get_knowledge_sources(tags?) -> Vec<KnowledgeSource>
delete_knowledge_source(source_id)
update_source_tags(source_id, tags)
search_knowledge_chunks(query, limit?, tags?) -> Vec<KnowledgeSearchResult>
link_knowledge_to_meeting(meeting_id, source_id)
get_meeting_knowledge(meeting_id) -> Vec<KnowledgeSource>
```

### Errors & Solutions

#### Error 1: MutexGuard Across Await
```
error: future cannot be sent between threads safely
help: within `impl Future<...>`, the trait `Send` is not implemented for `MutexGuard<...>`
```
**Cause:** Holding MutexGuard while calling async methods
**Solution:** Made WebCrawler stateless - create new instance per request instead of storing in AppState

#### Error 2: text-splitter API Changed
```
error[E0433]: failed to resolve: ChunkConfig not found
```
**Cause:** text-splitter v0.28 uses `Characters` type parameter instead of `usize`
**Solution:** Changed `MarkdownSplitter<usize>` to `MarkdownSplitter<Characters>`

#### Error 3: Spider Builder Lifetime Issues
```
error[E0716]: temporary value dropped while borrowed
```
**Cause:** Spider's builder pattern borrows from intermediate values
**Solution:** Switched to using `reqwest` directly for single-page fetching (simpler and avoids lifetime complexity)

#### Error 4: DuckDuckGo Search API
```
error[E0308]: mismatched types - expected `&str`, found `&[&str; 1]`
```
**Cause:** Misread API - `search()` takes `&str` not `&[str]`
**Solution:** Just pass query directly: `search.search(query).await`

### Files Created
- `/app/src-tauri/src/chunker.rs` - Text chunking module
- `/app/src-tauri/src/web_crawler.rs` - Web search and crawl module

### Files Modified
- `/app/src-tauri/Cargo.toml` - Added spider, duckduckgo_search, text-splitter
- `/app/src-tauri/src/lib.rs` - Added modules, imports, 9 new Tauri commands
- `/app/src-tauri/src/knowledge_base.rs` - Added structs, schema, CRUD methods

### Compilation Status
- **Rust**: 0 errors, 8 warnings (unused code)
- Still need: LLM tools, frontend component

### LLM Agent Tools Added

Added three new tools to `/app/src-tauri/src/llm_agent.rs`:

#### WebSearchTool
```rust
pub struct WebSearchTool;

impl Tool for WebSearchTool {
    const NAME: &'static str = "web_search";
    // Searches DuckDuckGo and returns titles + URLs
}
```

#### CrawlUrlTool
```rust
pub struct CrawlUrlTool {
    pub kb: Arc<RwLock<Option<KnowledgeBase>>>,
}

impl Tool for CrawlUrlTool {
    const NAME: &'static str = "crawl_url";
    // Crawls URL, converts to markdown, stores in KB with chunks
}
```

#### SearchKnowledgeTool
```rust
pub struct SearchKnowledgeTool {
    pub kb: Arc<RwLock<Option<KnowledgeBase>>>,
}

impl Tool for SearchKnowledgeTool {
    const NAME: &'static str = "search_knowledge";
    // Vector search across knowledge chunks with tag filtering
}
```

**Updated MeetingAssistant.ask()** to include all 5 tools:
- search_transcripts (existing)
- get_action_items (existing)
- web_search (new)
- crawl_url (new)
- search_knowledge (new)

### Frontend Component Created

**New File: `/app/src/lib/components/KnowledgeBaseView.svelte`**

Full-featured Knowledge Base management UI with:

**Three Tabs:**
1. **Sources** - Grid view of stored knowledge sources with:
   - Title, URL, type, date, tags
   - Delete button (on hover)
   - Empty state with "Add your first source" CTA

2. **Search** - Two search modes:
   - **Knowledge Base Search** - Vector similarity search across stored chunks
   - **Web Search** - DuckDuckGo search with "Add" button to store results

3. **Add Source** - Form to add new URLs:
   - URL input
   - Tags input (comma-separated)
   - Crawl & Store button with loading state
   - Tips sidebar

**Features:**
- Filter by tags (header input)
- Loading overlay with spinner
- Success/error messages (auto-dismiss)
- Responsive grid layout
- Dark mode styling matching SecondBrain theme

### SecondBrain Navigation Integration

**Modified `/app/src/lib/components/SecondBrain.svelte`:**

1. Added import for KnowledgeBaseView
2. Extended navigation type to include 'knowledge'
3. Added navigation button with ğŸ“š icon
4. Added KnowledgeBaseView in main content area

**Navigation Order:**
```
ğŸ  Home
ğŸ“… Meetings (with LIVE badge)
ğŸ“ Notes
ğŸ’¡ Insights
ğŸ“š Knowledge Base  <-- NEW
ğŸ”Œ Integrations
```

### Compilation Status
- **TypeScript:** 0 errors, 1 warning (a11y)
- **Rust:** 0 errors, 11 warnings (unused code)

### Architecture Summary

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Web Crawler Agent Flow                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  User/LLM                                                           â”‚
â”‚     â”‚                                                               â”‚
â”‚     â”œâ”€â–º web_search(query) â”€â”€â”€â”€â”€â–º DuckDuckGo API â”€â–º Results         â”‚
â”‚     â”‚                                                               â”‚
â”‚     â”œâ”€â–º crawl_url(url) â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º reqwest â”€â–º HTML                  â”‚
â”‚     â”‚                               â”‚                               â”‚
â”‚     â”‚                               â–¼                               â”‚
â”‚     â”‚                          html_to_markdown()                   â”‚
â”‚     â”‚                               â”‚                               â”‚
â”‚     â”‚                               â–¼                               â”‚
â”‚     â”‚                          DocumentChunker                      â”‚
â”‚     â”‚                               â”‚                               â”‚
â”‚     â”‚                               â–¼                               â”‚
â”‚     â”‚                          EmbeddingEngine                      â”‚
â”‚     â”‚                               â”‚                               â”‚
â”‚     â”‚                               â–¼                               â”‚
â”‚     â”‚                          SurrealDB                            â”‚
â”‚     â”‚                          â”œâ”€â”€ knowledge_source                 â”‚
â”‚     â”‚                          â””â”€â”€ knowledge_chunk (with vectors)   â”‚
â”‚     â”‚                                                               â”‚
â”‚     â””â”€â–º search_knowledge(query) â”€â–º Vector similarity search        â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Files Created
- `/app/src/lib/components/KnowledgeBaseView.svelte` - Full KB management UI

### Files Modified
- `/app/src-tauri/src/llm_agent.rs` - Added 3 new tools, updated MeetingAssistant
- `/app/src/lib/components/SecondBrain.svelte` - Added KB navigation and view

---

## Session 11 (continued) - Web Crawler Agent Complete

### Document Processing Pipeline

The web crawler uses a complete document processing pipeline:

#### 1. HTML to Markdown Conversion (`web_crawler.rs`)
```rust
fn html_to_markdown(html: &str) -> String {
    // Remove unwanted tags (script, style, nav, footer, header)
    // Convert semantic elements:
    //   <h1> â†’ # , <h2> â†’ ## , etc.
    //   <p> â†’ \n\n
    //   <li> â†’ \n-
    //   <strong>/<b> â†’ **text**
    //   <em>/<i> â†’ *text*
    //   <code> â†’ `text`
    //   <pre> â†’ ```\ntext\n```
    //   <a href="url">text</a> â†’ [text](url)
    // Decode HTML entities (&nbsp;, &amp;, etc.)
    // Clean up whitespace
}
```

#### 2. Semantic Chunking (`chunker.rs`)
```rust
pub struct DocumentChunker {
    config: ChunkerConfig,  // chunk_size: 1000 chars (~250 tokens)
    splitter: MarkdownSplitter<Characters>,
}

impl DocumentChunker {
    pub fn chunk_markdown(&self, content: &str) -> Vec<Chunk> {
        // Uses text-splitter crate with markdown feature
        // Preserves semantic boundaries:
        //   - Paragraph breaks
        //   - Sentence endings
        //   - Heading boundaries
        //   - Code block integrity
        // Returns chunks with position metadata
    }
}
```

#### 3. Embedding Generation (`knowledge_base.rs`)
```rust
// In add_knowledge_source():
for chunk in chunks {
    // EmbeddingGemma-300M generates 768-dim vectors
    let embedding = self.embedding_engine.embed(&chunk.text)?;

    let kb_chunk = KnowledgeChunk {
        source_id: source_id.clone(),
        text: chunk.text,
        chunk_index: chunk.chunk_index as i32,
        embedding,  // Vec<f32> with 768 dimensions
    };

    self.db.create("knowledge_chunk").content(kb_chunk).await?;
}
```

#### 4. Vector Search (`knowledge_base.rs`)
```rust
pub async fn search_knowledge(&self, query: &str, limit: usize, tags: Option<Vec<String>>) {
    // Generate query embedding
    let query_embedding = self.embedding_engine.embed(query)?;

    // SurrealDB vector similarity search
    let chunks = self.db.query(r#"
        SELECT *, vector::similarity::cosine(embedding, $embedding) AS similarity
        FROM knowledge_chunk
        WHERE source_id IN (
            SELECT id FROM knowledge_source WHERE tags CONTAINSANY $tags
        )
        ORDER BY similarity DESC
        LIMIT $limit
    "#).bind(("embedding", query_embedding))...
}
```

### LLM Integration

The web crawler tools are available to the self-hosted LLM:

```rust
// MeetingAssistant configuration
let client = openai::Client::from_url(
    "https://lmstudio.subh-dev.xyz/llm/v1",  // Your self-hosted endpoint
    "dummy-key"
);

let agent = client
    .agent("openai/gpt-oss-20b")  // Your model
    .preamble("You are an intelligent meeting assistant...")
    .tool(SearchTranscriptsTool { kb })      // Search past meetings
    .tool(GetActionItemsTool { kb })         // Get action items
    .tool(WebSearchTool)                      // DuckDuckGo search
    .tool(CrawlUrlTool { kb })               // Crawl & store URLs
    .tool(SearchKnowledgeTool { kb })        // Search stored knowledge
    .temperature(0.7)
    .build();
```

### Complete Data Flow

#### Manual Mode (Pre-meeting prep via UI)
```
KnowledgeBaseView.svelte
    â”‚
    â”œâ”€â–º "Add Source" tab
    â”‚       â””â”€â–º User enters URL + tags
    â”‚               â””â”€â–º invoke("crawl_and_store", { url, tags })
    â”‚
    â””â”€â–º "Search" tab
            â”œâ”€â–º Web Search â†’ DuckDuckGo results â†’ "Add" button
            â””â”€â–º KB Search â†’ Vector similarity â†’ Results with source info
```

#### Agent Mode (During meetings)
```
User: "Find documentation about React hooks and save it"
    â”‚
    â–¼
MeetingAssistant.ask(question, kb)
    â”‚
    â”œâ”€â–º LLM analyzes question
    â”‚
    â”œâ”€â–º LLM calls web_search("React hooks documentation")
    â”‚       â””â”€â–º Returns: [{title, url}, ...]
    â”‚
    â”œâ”€â–º LLM calls crawl_url(url, tags=["react", "docs"], store=true)
    â”‚       â”œâ”€â–º Fetch HTML
    â”‚       â”œâ”€â–º Convert to Markdown
    â”‚       â”œâ”€â–º Chunk into ~1000 char segments
    â”‚       â”œâ”€â–º Generate 768-dim embedding per chunk
    â”‚       â”œâ”€â–º Store in SurrealDB
    â”‚       â””â”€â–º Returns: Preview + source_id
    â”‚
    â””â”€â–º LLM returns summary to user
```

#### Search Mode (Retrieving stored knowledge)
```
User: "What did we store about authentication?"
    â”‚
    â–¼
MeetingAssistant.ask(question, kb)
    â”‚
    â”œâ”€â–º LLM calls search_knowledge("authentication", limit=5)
    â”‚       â”œâ”€â–º Generate query embedding
    â”‚       â”œâ”€â–º Cosine similarity search in SurrealDB
    â”‚       â””â”€â–º Returns: [{chunk_text, source_title, source_url, similarity}, ...]
    â”‚
    â””â”€â–º LLM synthesizes answer from retrieved chunks
```

### Storage Schema

```
SurrealDB Tables:
â”œâ”€â”€ knowledge_source
â”‚   â”œâ”€â”€ id (auto)
â”‚   â”œâ”€â”€ url (unique index)
â”‚   â”œâ”€â”€ title
â”‚   â”œâ”€â”€ source_type ("web", "file", "search")
â”‚   â”œâ”€â”€ raw_content (full markdown)
â”‚   â”œâ”€â”€ tags[] (indexed)
â”‚   â”œâ”€â”€ created_at
â”‚   â””â”€â”€ last_updated
â”‚
â”œâ”€â”€ knowledge_chunk
â”‚   â”œâ”€â”€ id (auto)
â”‚   â”œâ”€â”€ source_id (foreign key, indexed)
â”‚   â”œâ”€â”€ text (~1000 chars)
â”‚   â”œâ”€â”€ chunk_index
â”‚   â””â”€â”€ embedding (768-dim float vector)
â”‚
â””â”€â”€ meeting_knowledge (links)
    â”œâ”€â”€ meeting_id
    â”œâ”€â”€ source_id
    â”œâ”€â”€ relevance_score
    â””â”€â”€ assigned_by ("user" or "auto")
```

---

## Session 12 - Knowledge Base Fixes & Document Upload

### Issues Fixed

#### 1. "Knowledge base not initialized" Error
The KnowledgeBaseView component wasn't initializing the required engines before use.

**Fix:** Added initialization sequence in `KnowledgeBaseView.svelte`:
```typescript
async function initializeKB() {
  await invoke("initialize_entities");      // GLiNER
  await invoke("initialize_embeddings");    // EmbeddingGemma
  await invoke("initialize_knowledge_base"); // SurrealDB
  kbInitialized = true;
  await loadSources();
}
```

Added UI feedback:
- Init status banner showing current step
- Warning message if initialization fails with retry button
- Disabled buttons when KB not ready

### Document Upload Support Added

#### Frontend (`KnowledgeBaseView.svelte`)
- Added Tauri dialog plugin for file picker
- New upload zone UI with drag/drop styling
- Supports PDF, TXT, MD files
- Upload status indicator

```typescript
async function uploadDocument() {
  const selected = await open({
    filters: [{ name: 'Documents', extensions: ['pdf', 'txt', 'md'] }]
  });
  const sourceId = await invoke("upload_document", { filePath, tags });
}
```

#### Backend (`lib.rs`)
New `upload_document` Tauri command:
```rust
#[tauri::command]
async fn upload_document(
    state: tauri::State<'_, AppState>,
    file_path: String,
    tags: Vec<String>,
) -> Result<String, String> {
    let content = match extension.as_str() {
        "txt" | "md" | "markdown" => fs::read_to_string(&file_path)?,
        "pdf" => extract_pdf_text(&file_path)?,
        _ => return Err("Unsupported file type"),
    };

    kb.add_knowledge_source(
        &format!("file://{}", file_path),
        &file_name,
        &content,
        source_type,
        tags,
    ).await
}

fn extract_pdf_text(file_path: &str) -> Result<String, String> {
    pdf_extract::extract_text_from_mem(&std::fs::read(file_path)?)
}
```

### New Dependencies

**Rust (`Cargo.toml`):**
```toml
pdf-extract = "0.7"
tauri-plugin-dialog = "2"
tauri-plugin-fs = "2"
```

**JavaScript (`package.json`):**
```json
"@tauri-apps/plugin-dialog": "^2.4.2",
"@tauri-apps/plugin-fs": "^2.4.4"
```

### Configuration Updates

**`tauri.conf.json`:**
```json
"plugins": {
  "dialog": {},
  "fs": {
    "scope": {
      "allow": ["$HOME/**", "$DOCUMENT/**", "$DOWNLOAD/**", "**"]
    }
  }
}
```

**`capabilities/default.json`:**
```json
"permissions": [
  "core:default",
  "opener:default",
  "dialog:default",
  "fs:default",
  "fs:allow-read-file",
  "fs:allow-read-text-file"
]
```

### UI Improvements

- Redesigned "Add Source" tab with side-by-side URL and Document upload forms
- Upload zone with dashed border and hover effects
- Initialization status banner with spinner
- Retry button when initialization fails
- Better source type icons (ğŸŒ web, ğŸ“„ file, ğŸ“• pdf)

### Files Modified
- `/app/src/lib/components/KnowledgeBaseView.svelte` - Init sequence, upload UI
- `/app/src-tauri/src/lib.rs` - `upload_document` command, plugin registration
- `/app/src-tauri/Cargo.toml` - Added pdf-extract, dialog, fs plugins
- `/app/src-tauri/tauri.conf.json` - Plugin configuration
- `/app/src-tauri/capabilities/default.json` - Permissions
- `/app/package.json` - Dialog and fs plugin packages

### Next Steps
1. Test end-to-end with real URLs and PDFs
2. Add auto-relevance detection (link KB sources to meetings automatically)
3. Add progress events for long crawl/upload operations
4. Add OCR support for scanned PDFs (optional)

---

## Session 13 - AI Q&A, RAG Implementation & Bug Fixes

### Overview

This session focused on adding AI-powered Q&A for the knowledge base, implementing Perplexity-style AI summaries for web search, and fixing multiple bugs that prevented the system from working correctly.

### What Was Built

#### 1. AI Q&A Interface for Knowledge Base

Added a new "Ask AI" tab to the KnowledgeBaseView component for RAG-based question answering.

**Frontend (`KnowledgeBaseView.svelte`):**
```typescript
// New state variables
let aiQuestion = $state("");
let aiAnswer = $state("");
let isAiThinking = $state(false);
let llmInitialized = $state(false);
let llmInitializing = $state(false);

// Initialize LLM on first use
async function initializeLLM() {
  await invoke("initialize_llm");
  llmInitialized = true;
}

// Ask AI with RAG
async function askAI() {
  const response = await invoke<string>("ask_assistant", { question: aiQuestion });
  aiAnswer = response;
}
```

**UI Features:**
- New "Ask AI" tab with robot emoji (ğŸ¤–)
- Text input for questions
- "Ask AI" button with loading states
- AI response display with styled card
- Example questions as clickable buttons
- Responsive two-column layout

#### 2. AI Summary for Web Search (Perplexity-style)

Added ability to summarize web search results using AI.

**New functionality:**
```typescript
let webSearchSummary = $state("");
let isSummarizing = $state(false);

async function summarizeWebSearch() {
  const resultsText = webSearchResults
    .map((r, i) => `${i + 1}. ${r.title}\nURL: ${r.url}\nSnippet: ${r.snippet}`)
    .join('\n\n');

  const prompt = `Based on these web search results for "${webSearchQuery}",
    provide a concise summary of the key information found.`;

  webSearchSummary = await invoke<string>("ask_assistant", { question: prompt });
}
```

**UI Features:**
- "âœ¨ AI Summary" gradient button after search results
- Styled summary card with purple/indigo gradient
- Auto-clear summary on new search

### Bugs Fixed

#### Bug 1: `from_url` Arguments Reversed

**Error:**
```
AI query failed: Failed to get response: CompletionError: HttpError: builder error: relative URL without a base
```

**Cause:** The rig-core `from_url` function signature is `(api_key, base_url)` but we were calling it as `(base_url, api_key)`.

**Fix (`llm_agent.rs`):**
```rust
// Before (wrong)
let client = openai::Client::from_url(api_url, "dummy-key");

// After (correct)
let client = openai::Client::from_url("dummy-key", api_url);
```

#### Bug 2: LLM Returning Raw Tool Output Instead of Answer

**Error:** AI responses showed raw search results instead of synthesized answers:
```
"**Source:** Welcome to Agno - Agno (https://docs.agno.com)\n**Relevance:** 55%\n**Content:**\n..."
```

**Cause:** The LLM model (`openai/gpt-oss-20b` via LM Studio) doesn't properly support the rig.rs agent tool-calling pattern. It just returns the tool output directly instead of using it to formulate an answer.

**Solution:** Replaced agent-based approach with simple RAG (Retrieval Augmented Generation):

```rust
/// Ask a question using RAG (Retrieval Augmented Generation)
/// This approach works better with models that don't support tool calling
pub async fn ask(
    &self,
    question: &str,
    kb: Arc<RwLock<Option<KnowledgeBase>>>,
) -> Result<String, String> {
    // Step 1: Search the knowledge base for relevant context
    let context = {
        let kb_guard = kb.read().await;
        if let Some(kb_ref) = kb_guard.as_ref() {
            let results = kb_ref.search_knowledge(question, 5, None).await.unwrap_or_default();
            // Format results as context string
            results.iter().map(|r| format!("Source: {} ({})\n{}", ...)).join("\n---\n")
        } else {
            String::new()
        }
    };

    // Step 2: Build prompt with context
    let prompt = format!(r#"
        You are a helpful assistant. Answer the user's question based on the provided context.

        CONTEXT FROM KNOWLEDGE BASE:
        {}

        USER QUESTION: {}

        ANSWER:"#, context, question);

    // Step 3: Get response from LLM (simple completion, no tools)
    let model = self.client.completion_model(&self.model);
    let response = model.completion_request(prompt).send().await?;
    Ok(extract_text(&response.choice.first()))
}
```

#### Bug 3: LLM Using Wrong Tool (search_transcripts instead of search_knowledge)

**Symptom:** AI returned "No relevant meeting segments found" even when content existed in knowledge base.

**Cause:** The LLM preamble emphasized meeting transcripts too heavily, causing it to use `search_transcripts` tool instead of `search_knowledge` tool.

**Fix:** Updated preamble (now moot after RAG refactor, but documented for reference):
```rust
.preamble(r#"
Your available tools:
1. search_knowledge - Search the knowledge base for stored web pages, documents, and content. USE THIS FIRST for most questions.
2. search_transcripts - Search through meeting transcripts specifically
...
IMPORTANT: ALWAYS use search_knowledge first - this searches all stored web pages, PDFs, and documents
"#)
.tool(knowledge_tool)  // Primary tool - listed first
.tool(search_tool)
```

#### Bug 4: Missing `CompletionModel` Trait Import

**Error:**
```
error[E0599]: no method named `completion_request` found for struct `rig::providers::openai::CompletionModel`
help: trait `CompletionModel` which provides `completion_request` is implemented but not in scope
```

**Fix:** Added trait import:
```rust
use rig::{
    completion::{AssistantContent, CompletionModel, Prompt, ToolDefinition},
    ...
};
```

#### Bug 5: `AssistantContent` Doesn't Implement `Display`

**Error:**
```
error[E0599]: no method named `to_string` found for enum `rig::completion::AssistantContent`
note: the following trait bounds were not satisfied:
    `rig::completion::AssistantContent: std::fmt::Display`
```

**Solution:** Created helper function to extract text from `AssistantContent` enum:
```rust
/// Extract text from AssistantContent
fn extract_text(content: &AssistantContent) -> String {
    match content {
        AssistantContent::Text(text_content) => text_content.text.clone(),
        AssistantContent::ToolCall(tool_call) => {
            format!("[Tool call: {}]", tool_call.function.name)
        }
    }
}

// Usage
Ok(extract_text(&response.choice.first()))
```

#### Bug 6: source_id Format Mismatch (Previous Session)

**Note:** This was fixed in a previous session but worth documenting. The `source_id` stored in chunks was the full Thing string (e.g., `knowledge_source:abc123`) but `get_knowledge_source` expected just the ID part.

**Fix in `knowledge_base.rs`:**
```rust
pub async fn get_knowledge_source(&self, source_id: &str) -> Result<Option<KnowledgeSource>, String> {
    // Extract just the ID part if full Thing string is passed
    let id_part = if source_id.starts_with("knowledge_source:") {
        source_id.strip_prefix("knowledge_source:").unwrap_or(source_id)
    } else {
        source_id
    };
    // ...
}
```

### Architecture: Entity Extraction & Graph-RAG Status

Confirmed current implementation status:

| Feature | Status | Notes |
|---------|--------|-------|
| Entity Extraction (GLiNER) | âœ… Implemented | Extracts people, orgs, topics, actions, etc. |
| Graph Storage (SurrealDB) | âœ… Implemented | Entities stored as nodes with `RELATES_TO` edges |
| Vector Search | âœ… Implemented | Cosine similarity search on 768-dim embeddings |
| Graph-RAG (Graph Traversal) | âŒ Not Yet | Search uses vectors only, not graph traversal |
| AI Q&A (RAG) | âœ… Implemented | Simple RAG with vector retrieval + LLM |

**Graph structure exists but isn't queried for RAG yet.** Current RAG uses vector similarity only.

### Files Modified

**`/app/src-tauri/src/llm_agent.rs`:**
- Fixed `from_url` argument order
- Replaced agent-based approach with simple RAG
- Added `extract_text()` helper function
- Added `CompletionModel` and `AssistantContent` imports
- Updated `suggest_questions()` to use same RAG pattern

**`/app/src/lib/components/KnowledgeBaseView.svelte`:**
- Added AI Q&A state variables and functions
- Added new "Ask AI" tab to navigation
- Added AI Q&A UI with input, button, response display
- Added example questions as clickable buttons
- Added `summarizeWebSearch()` function
- Added AI Summary button and display for web search
- Added extensive CSS for new UI elements
- Fixed a11y issues (changed `<li onclick>` to `<button>`)

### New CSS Styles Added

```css
/* Ask AI Section */
.ask-section { display: grid; grid-template-columns: 1fr 300px; gap: 1.5rem; }
.ask-container { background: rgba(255, 255, 255, 0.02); border-radius: 1rem; padding: 1.5rem; }
.ask-input { flex: 1; padding: 0.875rem 1rem; border-radius: 0.5rem; }
.ai-answer { margin-top: 1.5rem; background: rgba(99, 102, 241, 0.05); border-radius: 0.75rem; }
.answer-header { background: rgba(99, 102, 241, 0.1); padding: 0.75rem 1rem; }
.example-q { width: 100%; text-align: left; padding: 0.75rem; cursor: pointer; }
.example-q:hover { background: rgba(99, 102, 241, 0.1); color: #a5b4fc; }

/* Web Search Summary */
.summary-bar { display: flex; align-items: center; gap: 0.75rem; margin-top: 1rem; }
.summarize-btn { background: linear-gradient(135deg, #6366f1, #8b5cf6); }
.web-summary { background: linear-gradient(135deg, rgba(99, 102, 241, 0.1), rgba(139, 92, 246, 0.1)); }
.summary-header { background: rgba(139, 92, 246, 0.1); }
```

### How It Works Now

#### AI Q&A Flow
```
User types question in "Ask AI" tab
    â†“
askAI() called
    â†“
invoke("ask_assistant", { question })
    â†“
MeetingAssistant.ask() in Rust:
    1. Search knowledge base for relevant chunks (vector similarity)
    2. Build RAG prompt with context + question
    3. Send to LLM (simple completion, no tools)
    4. Extract text from response
    â†“
Display answer in UI
```

#### Web Search Summary Flow
```
User searches web (DuckDuckGo)
    â†“
Results displayed
    â†“
User clicks "âœ¨ AI Summary"
    â†“
summarizeWebSearch() formats results as context
    â†“
invoke("ask_assistant", { question: prompt })
    â†“
LLM generates summary of search results
    â†“
Summary displayed above results
```

### Compilation Status
- **TypeScript:** 0 errors, 1 warning (a11y in SecondBrain.svelte)
- **Rust:** 0 errors, 26 warnings (unused code)

---

## Session 14 - AI Q&A Empty KB Handling Fix

### Issue Identified
User reported AI returning "I don't have any knowledge-base entries available" when asking questions about the knowledge base, even though the AI was responding successfully.

### Root Cause
1. **KB Not Initialized Check Missing:** The `askAI()` function in the frontend only checked if `llmInitialized` was true, but not if `kbInitialized` was true. The LLM could be ready while the knowledge base wasn't.
2. **Empty KB Causing Wasted LLM Calls:** When the knowledge base was empty or had no matching content, the code still called the LLM with an "empty context" prompt, which was wasteful and gave poor user feedback.

### Fixes Applied

#### Fix 1: Frontend KB Initialization Check

**File:** `/app/src/lib/components/KnowledgeBaseView.svelte`

Added check to ensure KB is initialized before asking questions:
```typescript
async function askAI() {
  if (!aiQuestion.trim() || isAiThinking) return;

  // NEW: Ensure knowledge base is initialized first
  if (!kbInitialized) {
    error = "Knowledge base is not initialized yet. Please wait for initialization to complete.";
    return;
  }

  // Initialize LLM if needed
  if (!llmInitialized) {
    await initializeLLM();
    if (!llmInitialized) return;
  }
  // ... rest of function
}
```

#### Fix 2: Early Return with Helpful Message When KB Empty

**File:** `/app/src-tauri/src/llm_agent.rs`

Instead of calling the LLM when there's no context, return early with a helpful message:
```rust
pub async fn ask(&self, question: &str, kb: ...) -> Result<String, String> {
    println!("[RAG] Asking question: {}", question);

    let context = {
        let kb_guard = kb.read().await;
        if let Some(kb_ref) = kb_guard.as_ref() {
            println!("[RAG] Knowledge base found, searching...");
            let results = kb_ref.search_knowledge(question, 5, None).await.unwrap_or_default();
            println!("[RAG] Found {} results", results.len());
            // ... format results
        } else {
            println!("[RAG] Knowledge base NOT initialized!");
            String::new()
        }
    };

    // NEW: Early return with helpful message instead of calling LLM
    if context.is_empty() {
        println!("[RAG] No context found, sending empty KB response");
        return Ok("I couldn't find any relevant information in your knowledge base...

**Possible reasons:**
- Your knowledge base might be empty. Try adding some content first.
- The question might not match any stored content.

**To add content:**
1. Go to the \"Add Source\" tab
2. Add a URL to crawl, or upload a document
3. Then try asking your question again!".to_string());
    }
    // ... rest of function (only called when context exists)
}
```

### Benefits of These Fixes
1. **Better User Feedback:** User immediately knows why the AI can't answer (KB not ready or empty)
2. **Reduced Latency:** No unnecessary LLM call when there's nothing to answer from
3. **Helpful Instructions:** User gets clear guidance on how to add content
4. **Debug Logging:** `[RAG]` prefix logs help diagnose issues

### Files Modified
- `/app/src/lib/components/KnowledgeBaseView.svelte` - Added `kbInitialized` check in `askAI()`
- `/app/src-tauri/src/llm_agent.rs` - Added debug logging and early return with helpful message

### Compilation Status
- **Rust:** 0 errors, 26 warnings (unchanged)

---

## Session 15 - Delete Bug Fix, Orphan Cleanup & Source ID Mismatch

### Issues Identified

1. **AI Q&A found chunks but returned 0 results** - Vector search found 5 relevant chunks but all were filtered out because source lookup failed
2. **Delete not removing chunks** - Deleting a source left orphaned chunks in the database
3. **Orphaned data from old deleted sources** - Previously deleted PDFs still had chunks in the database

### Root Cause Analysis

The `source_id` format mismatch was the core issue:

| Location | Format | Example |
|----------|--------|---------|
| Stored in chunks | Full Thing string | `knowledge_source:xyz123` |
| Frontend sends to delete | Just ID part | `xyz123` |
| `get_knowledge_source` expects | Just ID part | `xyz123` |

When chunks stored `knowledge_source:xyz123` but delete searched for `xyz123`, nothing matched.

### Fixes Applied

#### Fix 1: Return Results Even When Source Lookup Fails

**File:** `/app/src-tauri/src/knowledge_base.rs` - `search_knowledge()`

```rust
// Before: Skipped chunks when source not found
Ok(None) => {
    println!("Warning: No source found for source_id={}", chunk_sim.source_id);
}

// After: Include chunk with fallback metadata
let (source_title, source_url) = match self.get_knowledge_source(&chunk_sim.source_id).await {
    Ok(Some(source)) => (source.title, source.url),
    Ok(None) => {
        println!("Warning: No source found for source_id={}, using fallback", chunk_sim.source_id);
        (format!("Source {}", chunk_sim.source_id), String::new())
    }
    Err(e) => {
        println!("Error getting source: {}, using fallback", e);
        (format!("Source {}", chunk_sim.source_id), String::new())
    }
};
results.push(KnowledgeSearchResult { chunk, source_title, source_url, similarity });
```

#### Fix 2: Delete Function Handles Both ID Formats

**File:** `/app/src-tauri/src/knowledge_base.rs` - `delete_knowledge_source()`

```rust
pub async fn delete_knowledge_source(&self, source_id: &str) -> Result<(), String> {
    // Generate both formats
    let full_source_id = if source_id.starts_with("knowledge_source:") {
        source_id.to_string()
    } else {
        format!("knowledge_source:{}", source_id)
    };

    let id_part = source_id.strip_prefix("knowledge_source:")
        .unwrap_or(source_id).to_string();

    // Delete chunks matching EITHER format
    self.db
        .query("DELETE FROM knowledge_chunk WHERE source_id = $full_id OR source_id = $short_id")
        .bind(("full_id", full_source_id.clone()))
        .bind(("short_id", id_part.clone()))
        .await?;

    // Delete the source itself
    self.db.delete::<Option<KnowledgeSource>>(("knowledge_source", id_part.as_str())).await?;
    Ok(())
}
```

#### Fix 3: Orphan Cleanup Function

**File:** `/app/src-tauri/src/knowledge_base.rs`

```rust
pub async fn cleanup_orphaned_chunks(&self) -> Result<usize, String> {
    // Get unique source_ids using GROUP BY (SurrealDB syntax, not DISTINCT)
    let chunk_source_ids: Vec<serde_json::Value> = self.db
        .query("SELECT source_id FROM knowledge_chunk GROUP BY source_id")
        .await?
        .take(0)?;

    let mut deleted_count = 0;
    for row in chunk_source_ids {
        if let Some(source_id) = row.get("source_id").and_then(|v| v.as_str()) {
            if self.get_knowledge_source(source_id).await?.is_none() {
                self.db
                    .query("DELETE FROM knowledge_chunk WHERE source_id = $source_id")
                    .bind(("source_id", source_id.to_string()))
                    .await?;
                deleted_count += 1;
            }
        }
    }
    Ok(deleted_count)
}
```

#### Fix 4: UI Cleanup Button (Tailwind)

**File:** `/app/src/lib/components/KnowledgeBaseView.svelte`

```svelte
<div class="flex justify-end mb-3">
  <button
    class="px-3 py-1.5 text-xs rounded-lg bg-white/10 text-white/80 hover:bg-white/15 disabled:opacity-50 disabled:cursor-not-allowed transition-colors"
    onclick={cleanupOrphanedChunks}
    disabled={isLoading}
    title="Remove orphaned chunks from deleted sources"
  >
    ğŸ§¹ Cleanup Orphans
  </button>
</div>
```

### Bug Fixed: SurrealDB DISTINCT Syntax

**Error:**
```
Parse error: Unexpected token `an identifier`, expected FROM --> [1:17]
| 1 | SELECT DISTINCT source_id FROM knowledge_chunk
```

**Fix:** SurrealDB uses `GROUP BY` instead of `DISTINCT`:
```sql
-- Wrong (SQL syntax)
SELECT DISTINCT source_id FROM knowledge_chunk

-- Correct (SurrealDB syntax)
SELECT source_id FROM knowledge_chunk GROUP BY source_id
```

### Files Modified

| File | Changes |
|------|---------|
| `knowledge_base.rs` | Fixed `delete_knowledge_source()`, added `cleanup_orphaned_chunks()`, fixed `search_knowledge()` fallback |
| `lib.rs` | Added `cleanup_orphaned_chunks` Tauri command |
| `KnowledgeBaseView.svelte` | Added cleanup button with Tailwind classes |

### Design Note

Using Tailwind CSS (v4) instead of custom CSS for new UI elements. The project has Tailwind configured in `app.css` with `@import "tailwindcss"`.

### How to Clean Up Existing Orphaned Data

1. Open the app
2. Go to **Knowledge Base** â†’ **Sources** tab
3. Click **ğŸ§¹ Cleanup Orphans** button (top right)
4. Confirm the cleanup
5. Orphaned chunks from deleted sources will be removed

### Compilation Status
- **Rust:** 0 errors, 26 warnings

---

## Session 16 - December 2, 2025

### What Was Built

#### Full Graph-RAG Implementation

Replaced the pure vector-based RAG with a true Graph-RAG system that combines:
1. **Entity extraction** from queries using GLiNER multitask
2. **Relationship extraction** using GLiNER multitask (entity + relation extraction in one model)
3. **Temporal awareness** parsing time references like "3 weeks ago", "last month"
4. **Graph traversal** for related meetings, people, and topics
5. **Vector similarity search** for knowledge base chunks

### Model Upgrade: GLiNER Multitask

**Before:** `gliner_small-v2.1` (entities only)
**After:** `gliner-multitask-large-v0.5` (entities + relationships)

Source: `https://huggingface.co/onnx-community/gliner-multitask-large-v0.5`

Using `model_int8.onnx` (648MB quantized) for efficient inference.

### Files Modified

| File | Changes |
|------|---------|
| `models.rs` | Updated model URLs to use gliner-multitask-large-v0.5 |
| `entities.rs` | Complete rewrite to support both entity and relationship extraction |
| `knowledge_base.rs` | Added entity_relation table, process_relationships(), graph_rag_query(), temporal parsing |
| `llm_agent.rs` | Rewrote ask() to use Graph-RAG context instead of simple vector search |
| `Cargo.toml` | Added `regex` crate for temporal parsing |

### New Entity Extraction API

```rust
// Before: entities only
let entities = engine.extract(text)?;

// After: entities AND relationships
let (entities, relationships) = engine.extract_with_relations(text)?;
```

### Relationship Schema

The system extracts these relationship types:
- `discussed` - Person â†’ Topic/Project/Product
- `assigned_to` - ActionItem â†’ Person
- `decided` - Person â†’ Decision
- `mentioned` - Person â†’ Person/Organization/Topic/Project
- `works_on` - Person â†’ Project/Product
- `reported` - Person â†’ Metric
- `belongs_to` - Person â†’ Organization
- `deadline_for` - Deadline â†’ ActionItem/Project
- `related_to` - Topic/Project â†’ Topic/Project/Product
- `located_in` - Organization/Person â†’ Location

### Graph-RAG Query Flow

```
User Question â†’ 
  1. Extract entities from query (GLiNER)
  2. Parse temporal references (regex)
  3. Graph traversal:
     - Get related meetings (filtered by time if temporal)
     - Get related people (from entity_relation table)
     - Get related topics
  4. Get open action items
  5. Get recent decisions
  6. Vector search for similar chunks
  â†’ Combined context sent to LLM
```

### Temporal Parsing

Supports:
- `N weeks ago` â†’ timestamp range
- `N days ago` â†’ timestamp range
- `last week` â†’ 7-14 days ago
- `last month` â†’ 0-30 days ago
- `yesterday` â†’ 1-2 days ago

### New Database Schema

```sql
DEFINE TABLE entity_relation SCHEMAFULL;
DEFINE FIELD source_entity ON entity_relation TYPE string;
DEFINE FIELD source_type ON entity_relation TYPE string;
DEFINE FIELD relation ON entity_relation TYPE string;
DEFINE FIELD target_entity ON entity_relation TYPE string;
DEFINE FIELD target_type ON entity_relation TYPE string;
DEFINE FIELD confidence ON entity_relation TYPE float;
DEFINE FIELD meeting_id ON entity_relation TYPE option<string>;
DEFINE FIELD created_at ON entity_relation TYPE int;
DEFINE INDEX idx_relation_source ON entity_relation FIELDS source_entity;
DEFINE INDEX idx_relation_target ON entity_relation FIELDS target_entity;
DEFINE INDEX idx_relation_type ON entity_relation FIELDS relation;
```

### AI Q&A Context Format

The LLM now receives rich context:

```markdown
## Temporal Reference Detected
Time reference: 3 weeks ago

## Entities Mentioned in Query
Project Alpha (project), John Smith (person)

## Related Meetings
**Weekly Standup** (21 days ago)
  - John: "We need to finalize Project Alpha..."

## Related People
- **John Smith** (last seen 5 days ago): discusses Project Alpha, Budget

## Related Topics
- **Project Alpha**: mentioned 15 times, last 5 days ago (discussed by: John, Alice)

## Open Action Items
- Review Project Alpha proposal (assigned to: John)

## Recent Decisions
- Approved Phase 2 of Project Alpha

## Knowledge Base Content
**Source:** Project Alpha Docs (https://...)
**Relevance:** 87%
...content...
```

### Example Queries That Now Work

1. "What did John say about Project Alpha 3 weeks ago?"
2. "Show me action items assigned to Alice"
3. "What decisions were made last month about the budget?"
4. "Who has been discussing the marketing strategy?"

### Compilation Status
- **Rust:** 0 errors, 29 warnings (unused code from earlier sessions)

### Next Steps

1. Download the new GLiNER multitask model (user needs to re-run model download)
2. Test temporal queries with actual meeting data
3. Consider adding more relationship types based on usage

---

## Session 16 Addendum - Model Mismatch Fix

### Issue Encountered

After implementing Graph-RAG, the entity extraction failed with:
```
Inference failed: input tensors mismatch: pipeline expects 
{"attention_mask", "input_ids", "text_lengths", "span_idx", "span_mask", "words_mask"} 
but model has {"attention_mask", "input_ids", "text_lengths", "words_mask"}
```

### Root Cause

The error message from orp crate has swapped labels (bug in error formatting). The actual situation:
- **Model file** (old `gliner_small-v2.1`, 175MB): Uses **span-mode** with 6 input tensors
- **Code** (new TokenPipeline): Expects **token-mode** with 4 input tensors

The model download URLs were updated to the new multitask model, but the old model file already existed in `~/Library/Application Support/second-brain/models/` so it wasn't re-downloaded.

### Model Comparison

| Model | Size | Mode | Input Tensors |
|-------|------|------|---------------|
| `gliner_small-v2.1` (old) | 175MB | Span | 6 (includes `span_idx`, `span_mask`) |
| `gliner-multitask-large-v0.5` (new) | 648MB | Token | 4 (`input_ids`, `attention_mask`, `words_mask`, `text_lengths`) |

### Solution

Deleted the old model files to force re-download:
```bash
rm ~/Library/Application\ Support/second-brain/models/gliner-model.onnx
rm ~/Library/Application\ Support/second-brain/models/gliner-tokenizer.json
```

On next app launch, the new model will be downloaded:
- `gliner-model.onnx` (~648MB from `onnx-community/gliner-multitask-large-v0.5/onnx/model_int8.onnx`)
- `gliner-tokenizer.json` (~9MB)

### Note on orp Error Message Bug

The orp crate's error formatting swaps `expected` and `actual`:
```rust
// In orp-0.9.2/src/error.rs:13
format!("{} tensors mismatch: pipeline expects {:?} but model has {:?}", kind, actual, expected)
//                                                   ^^^^^^         ^^^^^^^^
//                                                   These are swapped!
```

This makes debugging confusing - "pipeline expects" actually shows what the model has, and "model has" shows what the pipeline expects.

### Fallback Behavior

If Graph-RAG fails (e.g., during model download), the system falls back to pure vector search:
```rust
match kb_ref.graph_rag_query(question, 5).await {
    Ok(graph_context) => { /* use Graph-RAG context */ }
    Err(e) => {
        println!("[Graph-RAG] Error: {}", e);
        // Fall back to simple vector search
        let results = kb_ref.search_knowledge(question, 5, None).await?;
        // ...
    }
}
```

---

## Session 16 - Known Issues (To Fix Later)

### Issue 1: SurrealDB Serialization Error

When querying knowledge sources, there's a serialization error:
```
[KB Search] Sources in DB: Err(Db(Serialization("invalid type: enum, expected any valid JSON value")))
[KB Search] Simple chunk query result: Err(Db(Serialization("invalid type: enum, expected any valid JSON value")))
```

**Impact:** The `SELECT * FROM knowledge_source` query fails to deserialize, but vector search still works.

**Likely Cause:** The `Thing` type (SurrealDB record ID) doesn't serialize cleanly to `serde_json::Value`. The query works but the Rust deserialization into our struct fails.

**Workaround:** Vector search still returns results (5 chunks found with similarity scores).

### Issue 2: Graph-RAG Returns Empty Graph Data

```
[Graph-RAG] Found 0 related meetings
[Graph-RAG] Found 0 related people  
[Graph-RAG] Found 0 related topics
```

**Cause:** 
1. No meetings have been created/recorded yet (only PDF was added as knowledge source)
2. Entity relationships are only extracted from meeting transcripts via `add_segment()`
3. Knowledge sources (PDFs, URLs) don't currently trigger entity/relationship extraction

**To Fix Later:**
- Add entity extraction when adding knowledge sources (not just meeting segments)
- Or: Create a separate ingestion path for documents that extracts entities

### Current Behavior

Despite the issues, the system still works for basic Q&A:
1. Entity extraction from query works: `[("knowledge base", "topic")]`
2. Vector search finds relevant chunks: 5 chunks with ~57% similarity
3. LLM receives context and can answer questions

The Graph-RAG enhancement (meetings, people, topics, relationships) requires:
- Active meeting recording with transcription
- Or: Document ingestion that also extracts entities

### Status: Partially Working

| Feature | Status |
|---------|--------|
| Entity extraction from query | âœ… Working |
| Temporal parsing | âœ… Working (but no temporal data yet) |
| Vector search | âœ… Working |
| Graph traversal (meetings) | âš ï¸ Empty (no meetings recorded) |
| Graph traversal (people) | âš ï¸ Empty (no entities extracted from docs) |
| Graph traversal (topics) | âš ï¸ Empty (no entities extracted from docs) |
| Relationship extraction | âš ï¸ Not triggered for knowledge sources |
| LLM response | âœ… Working (uses vector search results) |

---

## Session 16 - Fix: Entity Extraction for Knowledge Sources

### Problem

After adding a PDF, Graph-RAG returned 0 entities/topics/people because entity extraction only happened for meeting transcripts (`add_segment()`), not for knowledge sources (`add_knowledge_source()`).

### Solution

Added entity and relationship extraction when adding knowledge sources:

**File:** `knowledge_base.rs`

```rust
// In add_knowledge_source(), after chunking:
let text_chunks: Vec<&str> = content.split("\n\n")
    .filter(|s| s.len() > 50)
    .take(20)
    .collect();

for text_chunk in text_chunks {
    match self.entity_engine.extract_with_relations(text_chunk) {
        Ok((entities, relationships)) => {
            self.process_entities_for_source(&source_id, &entities).await.ok();
            self.process_relationships_for_source(&source_id, &relationships).await.ok();
        }
        Err(e) => println!("Entity extraction failed: {}", e);
    }
}
```

### New Methods Added

1. **`process_entities_for_source()`** - Upserts people and topics from knowledge sources
2. **`process_relationships_for_source()`** - Stores entity relationships with `knowledge_source_id`

### Schema Update

Added `knowledge_source_id` field to `entity_relation` table:
```sql
DEFINE FIELD knowledge_source_id ON entity_relation TYPE option<string>;
```

This allows relationships to be linked to either meetings OR knowledge sources.

### How It Works Now

When you add a PDF/URL:
1. Content is chunked and embedded (as before)
2. **NEW:** First 20 paragraphs (>50 chars) are processed for entity extraction
3. **NEW:** People â†’ stored in `person` table
4. **NEW:** Topics/projects/products/orgs â†’ stored in `topic` table  
5. **NEW:** Relationships â†’ stored in `entity_relation` table

### Expected Log Output

```
Chunking content: 75997 chars -> 95 chunks
Added knowledge source: document.pdf (id=...) with 95 chunks
Extracted 47 entities and 12 relationships from knowledge source
```

### Compilation Status
- **Rust:** 0 errors, 30 warnings

---

## Session 16 - Fix: Graceful Relationship Extraction

### Problem

After adding entity extraction for knowledge sources, many errors appeared:
```
Entity extraction failed for chunk: Relation inference failed: invalid input: empty texts and/or entities
Entity extraction failed for chunk: Relation inference failed: unexpected relation label format
```

### Root Cause

1. Relationship extraction was running even when no entities were found
2. The relationship schema is designed for meeting-style relationships (person â†’ topic, action_item â†’ person), which don't match all document content
3. Errors in relationship extraction were failing the entire extraction process

### Solution

Made relationship extraction graceful - it tries but doesn't fail:

```rust
// Skip relationship extraction if no entities found
if entities.is_empty() {
    return Ok((entities, vec![]));
}

// Try relationship extraction, but don't fail if it errors
let relationships = match self.try_extract_relationships(entity_output, &entities) {
    Ok(rels) => rels,
    Err(_) => vec![], // Silently skip on error
};
```

### Result

Now you'll see:
```
Added knowledge source: document.pdf with 95 chunks
Extracted 6 entities and 0 relationships from knowledge source
```

Instead of all those error messages. The 6 entities ARE being extracted and stored - just relationships aren't found (which is fine for academic PDFs).

### Entity Types Being Extracted

From academic PDFs, you might see:
- `person` - Author names, researchers mentioned
- `organization` - Universities, companies, labs
- `topic` - Research areas, technologies
- `product` - Software, tools mentioned

### Compilation Status
- **Rust:** 0 errors, 30 warnings

---

## Session 16 - Remaining Issues (To Fix Later)

### Issue 1: SurrealDB Serialization Error (Low Priority)

```
[KB Search] Sources in DB: Err(Db(Serialization("invalid type: enum, expected any valid JSON value")))
[KB Search] Simple chunk query result: Err(Db(Serialization("invalid type: enum, expected any valid JSON value")))
```

**Location:** `knowledge_base.rs` in `search_knowledge()` method

**Cause:** SurrealDB's `Thing` type (record ID like `knowledge_source:abc123`) doesn't deserialize cleanly into `serde_json::Value`. When we do `SELECT * FROM knowledge_source`, the `id` field fails to serialize.

**Impact:** Low - Vector search still works, we just can't list all sources via that query.

**Potential Fix:** 
- Use a custom struct that handles `Thing` â†’ `String` conversion
- Or exclude the `id` field from SELECT: `SELECT url, title, source_type FROM knowledge_source`

---

### Issue 2: Graph-RAG Returns 0 Topics Despite Entities Being Extracted

```
Extracted 8 entities and 0 relationships from knowledge source
...
[Graph-RAG] Found 0 related topics
```

**Location:** `knowledge_base.rs` in `get_topic_context()` method

**Cause:** The `get_topic_context()` method looks for topics that match the **query's** extracted entities, not the document's entities. The query "What are the key topics?" only extracts `("knowledge base", "topic")`, which doesn't match any stored topic names.

**Current Flow:**
1. Query: "What are the key topics?" â†’ extracts entity `"knowledge base"`
2. `get_topic_context()` searches for topic WHERE name = "knowledge base"
3. No match found â†’ returns empty

**The 8 extracted entities from the PDF ARE stored** - they're just not being retrieved because the query doesn't mention them by name.

**Potential Fixes:**
1. Add a `get_all_topics()` method that returns top N topics regardless of query
2. Use vector similarity to find related topics instead of exact name match
3. Add a "show me all entities" type of query handling

---

### Issue 3: Query Doesn't Find Stored Entities

The query "What are the key topics in my knowledge base?" should ideally return the 8 entities extracted from the PDF, but it doesn't because:

1. Query entity extraction finds: `"knowledge base"` (generic term)
2. Graph lookup searches for: `topic WHERE name = "knowledge base"` 
3. PDF entities stored might be: `"neural networks"`, `"transformer"`, etc.
4. No match â†’ empty graph context

**Better Approach (Future):**
- For generic queries like "what topics do I have", bypass entity matching
- Return all topics sorted by mention_count or recency
- Or: embed the query and find topics with similar embeddings

---

### Current Status Summary

| Component | Status | Notes |
|-----------|--------|-------|
| PDF chunking | âœ… Working | 95 chunks created |
| Entity extraction | âœ… Working | 8 entities extracted |
| Entity storage | âœ… Working | Stored in person/topic tables |
| Vector search | âœ… Working | 5 chunks found with ~57% similarity |
| Graph query (topics) | âš ï¸ Returns empty | Query entities don't match stored entities |
| LLM response | âœ… Working | Gets vector search context |

### What Works Now

Despite the graph issues, the system DOES work for Q&A:
1. You ask a question
2. Vector search finds relevant chunks from your PDF
3. LLM gets those chunks as context
4. LLM responds based on the content

The Graph-RAG enhancement (showing related topics/people) needs the fixes above to work properly.

---

## TODO: Errors to Fix

### Error Log from Latest Run

```
Chunking content: 75997 chars -> 95 chunks
Added knowledge source: 2504.18425v1.pdf (id=knowledge_source:sl61foneemmjtbzw1gau) with 95 chunks
Extracted 8 entities and 0 relationships from knowledge source
LLM assistant initialized with model: openai/gpt-oss-20b
[Graph-RAG] Asking question: What are the key topics in my knowledge base?
[Graph-RAG] Knowledge base found, running Graph-RAG query...
[Graph-RAG] Query entities: [("knowledge base", "topic")]
[Graph-RAG] Temporal context: None
[Graph-RAG] Found 0 related meetings
[Graph-RAG] Found 0 related people
[Graph-RAG] Found 0 related topics
[KB Search] Raw count response: Some(Object {"count": Number(95)})
[KB Search] Count value: Some(Number(95))
[KB Search] Sources in DB: Err(Db(Serialization("invalid type: enum, expected any valid JSON value")))
[KB Search] Running vector search with embedding len=768
[KB Search] Simple chunk query result: Err(Db(Serialization("invalid type: enum, expected any valid JSON value")))
[KB Search] Vector query succeeded
Found 5 chunks with similarity
  Chunk: source_id=knowledge_source:sl61foneemmjtbzw1gau, text_len=48, similarity=0.5777
  Chunk: source_id=knowledge_source:sl61foneemmjtbzw1gau, text_len=336, similarity=0.5756
  Chunk: source_id=knowledge_source:sl61foneemmjtbzw1gau, text_len=322, similarity=0.5747
  Chunk: source_id=knowledge_source:sl61foneemmjtbzw1gau, text_len=892, similarity=0.5694
  Chunk: source_id=knowledge_source:sl61foneemmjtbzw1gau, text_len=683, similarity=0.5670
Returning 5 search results
[Graph-RAG] Found 5 similar chunks
```

### Summary of Issues to Fix

1. **SurrealDB Serialization Error** - `Thing` type doesn't serialize to JSON
2. **Graph-RAG returns 0 for meetings/people/topics** - Query entity matching doesn't find stored entities
3. **0 relationships extracted** - Relationship schema may not match document content

### Priority

- High: Fix graph context retrieval (should return stored entities)
- Medium: Fix SurrealDB serialization 
- Low: Improve relationship extraction for non-meeting content

---

## Session - December 5, 2025

### What Was Built

#### Meeting Recording Pipeline + Persistence + Meeting Page

Implemented the complete meeting flow: Live Recording â†’ Knowledge Base (entity extraction + embeddings) â†’ Meeting Detail Page.

### Backend Changes

#### 1. Knowledge Base Query Methods (`knowledge_base.rs`)

Added new methods for retrieving meeting data:

```rust
// Meeting retrieval
get_meetings(limit: Option<usize>) -> Vec<Meeting>
get_meeting(meeting_id: &str) -> Option<Meeting>
get_meeting_segments(meeting_id: &str) -> Vec<TranscriptSegment>
get_meeting_action_items(meeting_id: &str) -> Vec<ActionItem>
get_meeting_decisions(meeting_id: &str) -> Vec<Decision>
get_meeting_topics(meeting_id: &str) -> Vec<Topic>
get_meeting_people(meeting_id: &str) -> Vec<Person>
update_action_item_status(action_id: &str, status: &str)
get_meeting_stats(meeting_id: &str) -> MeetingStats
```

Added `MeetingStats` struct:
```rust
pub struct MeetingStats {
    pub segment_count: usize,
    pub action_count: usize,
    pub decision_count: usize,
    pub topic_count: usize,
    pub people_count: usize,
    pub duration_ms: u64,
    pub total_words: usize,
}
```

#### 2. New Tauri Commands (`lib.rs`)

Exposed 10 new commands to frontend:
- `get_meetings` - List all meetings
- `get_meeting` - Get single meeting by ID
- `get_meeting_segments` - Get transcript segments for a meeting
- `get_meeting_action_items` - Get action items for a meeting
- `get_meeting_decisions` - Get decisions for a meeting
- `get_meeting_topics` - Get topics discussed in a meeting
- `get_meeting_people` - Get people mentioned in a meeting
- `get_meeting_stats` - Get meeting statistics
- `update_action_item_status` - Update action item status
- `get_current_meeting_id` - Get current recording meeting ID

### Frontend Changes

#### 1. New Component: `MeetingDetailPage.svelte`

Full-featured meeting detail page with:
- **Header**: Meeting title, date, time, duration, stats (segments, words, actions, decisions)
- **Tabs**: Highlights | Transcript | Entities
- **Highlights Tab**:
  - Meeting summary (if available)
  - Action items with checkbox to mark done
  - Decisions list
  - Topics discussed as tags
  - People mentioned
- **Transcript Tab**: Full transcript with timestamps and speaker labels
- **Entities Tab**: Detailed view of extracted people and topics

#### 2. Updated `SecondBrain.svelte`

- Added `Meeting` interface for TypeScript
- Added `LiveSegment` interface for transcription events
- Added state for real meetings from KB and selected meeting
- Added live transcript display during recording
- Updated `startMeeting()` to clear live transcript
- Updated `endMeeting()` to reload meetings list
- **Home Page**: Recent meetings section now shows real data from KB
- **Meetings View**: Replaced mock data with real meetings, added search filter
- **Active Meeting Banner**: Now shows live transcript as it's being captured

#### 3. Live Transcript Feature

```typescript
// Listen for transcription events from Rust
unlistenTranscription = await listen<LiveSegment>("transcription", (event) => {
  const segment = event.payload;
  if (segment.is_final) {
    liveTranscript = [...liveTranscript, segment];
  }
});
```

Live transcript shows during recording with speaker labels (You vs Guest).

### Data Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         RECORDING FLOW                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  1. User clicks "Start Recording"                               â”‚
â”‚     â””â”€> invoke("start_meeting", { title, participants })        â”‚
â”‚         â””â”€> Creates Meeting in SurrealDB                        â”‚
â”‚             Returns meeting_id                                  â”‚
â”‚                                                                 â”‚
â”‚  2. Audio capture starts (mic + system)                         â”‚
â”‚     â””â”€> cpal captures audio samples                             â”‚
â”‚         â””â”€> Emitted to ASR engine                               â”‚
â”‚                                                                 â”‚
â”‚  3. ASR processes audio                                         â”‚
â”‚     â””â”€> Silero VAD detects speech                               â”‚
â”‚     â””â”€> Zipformer transcribes                                   â”‚
â”‚     â””â”€> Emits "transcription" event to frontend                 â”‚
â”‚                                                                 â”‚
â”‚  4. On final transcription:                                     â”‚
â”‚     â””â”€> Frontend receives via listen("transcription")           â”‚
â”‚         â””â”€> Updates live transcript display                     â”‚
â”‚     â””â”€> Backend saves to KB via add_segment()                   â”‚
â”‚         â””â”€> Generates embedding (EmbeddingGemma-300M)           â”‚
â”‚         â””â”€> Extracts entities (GLiNER)                          â”‚
â”‚         â””â”€> Creates graph relations                             â”‚
â”‚                                                                 â”‚
â”‚  5. User clicks "End Meeting"                                   â”‚
â”‚     â””â”€> invoke("end_meeting")                                   â”‚
â”‚         â””â”€> Updates meeting end_time                            â”‚
â”‚         â””â”€> Meeting appears in Meetings list                    â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        VIEWING FLOW                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  1. User opens Meetings view                                    â”‚
â”‚     â””â”€> invoke("get_meetings")                                  â”‚
â”‚         â””â”€> Returns Vec<Meeting> from SurrealDB                 â”‚
â”‚                                                                 â”‚
â”‚  2. User clicks on meeting card                                 â”‚
â”‚     â””â”€> Opens MeetingDetailPage with meetingId                  â”‚
â”‚         â””â”€> Parallel loads:                                     â”‚
â”‚             - get_meeting(meetingId)                            â”‚
â”‚             - get_meeting_segments(meetingId)                   â”‚
â”‚             - get_meeting_action_items(meetingId)               â”‚
â”‚             - get_meeting_decisions(meetingId)                  â”‚
â”‚             - get_meeting_topics(meetingId)                     â”‚
â”‚             - get_meeting_people(meetingId)                     â”‚
â”‚             - get_meeting_stats(meetingId)                      â”‚
â”‚                                                                 â”‚
â”‚  3. User can:                                                   â”‚
â”‚     - View full transcript with timestamps                      â”‚
â”‚     - See extracted highlights (actions, decisions, topics)     â”‚
â”‚     - Mark action items as done                                 â”‚
â”‚     - Navigate back to meetings list                            â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Files Modified

| File | Changes |
|------|---------|
| `knowledge_base.rs` | Added 10 new query methods + MeetingStats struct |
| `lib.rs` | Added 10 new Tauri commands for meeting retrieval |
| `SecondBrain.svelte` | Added real meeting data, live transcript, meeting detail navigation |
| `MeetingDetailPage.svelte` | **NEW** - Full meeting detail view with tabs |

### What Works Now

1. **Start Recording**: Creates meeting in KB, captures audio, transcribes in real-time
2. **Live Transcript**: Shows transcription during recording with speaker labels
3. **End Recording**: Saves meeting with all segments, entities, embeddings
4. **Meetings View**: Shows all recorded meetings with date, duration
5. **Meeting Detail**: Opens any meeting to see full transcript, action items, decisions, topics, people
6. **Action Item Toggle**: Can mark action items as done/open

### Pipeline Status

| Component | Status |
|-----------|--------|
| Audio capture â†’ ASR | âœ… Working |
| ASR â†’ KB persistence | âœ… Working (in add_segment) |
| Entity extraction | âœ… Working (GLiNER in add_segment) |
| Embedding generation | âœ… Working (EmbeddingGemma in add_segment) |
| Meetings list UI | âœ… Working |
| Meeting detail page | âœ… Working |
| Live transcript | âœ… Working |

### Next Steps

1. Add meeting summary generation (LLM)
2. Improve speaker diarization (currently just You/Guest)
3. Add search within meeting transcript
4. Add export functionality (PDF, Markdown)
5. Connect "Ask Your Brain" to query across all meetings

---

## Session 8 - December 5, 2025 - AI Integration & Diarization Research

### What Was Built

#### Phase 1: Fixed Initialization Order Bug
**Problem:** "Knowledge base not initialized" error when clicking Start Recording

**Root Cause:** The initialization sequence in `+page.svelte` was wrong:
- Knowledge Base requires Entity engine to be initialized first
- Entity and Embedding engines were initialized AFTER KB

**Solution:** Reordered initialization in `+page.svelte`:
```typescript
// Correct order:
1. User store (SQLite)
2. ASR engine
3. Embeddings engine
4. Entity engine      // MUST be before KB
5. Knowledge Base     // Requires entity engine
6. LLM Assistant      // NEW
7. Audio pipeline
```

**Additional Fix:** Added `appInitialized` state to prevent SecondBrain from loading data before backends are ready:
```svelte
{:else if !appInitialized}
  <div>Initializing engines...</div>
{:else}
  <SecondBrain {isRecording} />
{/if}
```

#### Phase 2: Debug Logging for KB Pipeline
Added comprehensive logging to trace transcript â†’ KB flow:

**In `lib.rs`:**
```rust
println!("[KB] Checking meeting_id: {:?}", meeting_id);
println!("[KB] Saving segment: speaker={}, text_len={}, meeting={}", ...);
println!("[KB] Segment saved successfully: {}", segment_id);
```

**In `knowledge_base.rs::add_segment()`:**
```rust
println!("[KB::add_segment] Starting for meeting={}, speaker={}, text_len={}", ...);
println!("[KB::add_segment] Generating embedding...");
println!("[KB::add_segment] Embedding generated, dim={}", ...);
println!("[KB::add_segment] Creating segment in DB...");
println!("[KB::add_segment] Extracting entities...");
println!("[KB::add_segment] Found {} entities, {} relationships", ...);
println!("[KB::add_segment] Success! Segment ID: {}", ...);
```

#### Phase 3: Ask AI Integration
Wired up the "Ask Your Brain" feature in the UI:

**Backend:** Already implemented in `lib.rs`:
- `initialize_llm(api_url, model)` - Initialize LLM assistant
- `ask_assistant(question)` - Query meetings with RAG
- `summarize_meeting(segments)` - Generate meeting summary
- `suggest_questions(topic)` - Get suggested questions

**Frontend Changes (`SecondBrain.svelte`):**
```typescript
// State
let aiResponse = $state<string>("");
let aiLoading = $state(false);

// Function
async function askAI() {
  aiLoading = true;
  const response = await invoke<string>("ask_assistant", { question: askQuery });
  aiResponse = response;
  aiLoading = false;
}
```

**UI Updates:**
- Enter key triggers ask
- Loading spinner while waiting
- Response display area
- Quick question buttons wired up

### Speaker Diarization Research

#### Current State
- Microphone audio â†’ labeled as "You"
- System audio â†’ labeled as "Guest" (all system audio = 1 speaker)
- No actual speaker diarization

#### Discovery: sherpa-rs Has Built-in Diarization!

Already in our dependency (`sherpa-rs = "0.6"`):
```rust
// From sherpa-rs/src/diarize.rs
pub struct Diarize { ... }

pub struct Segment {
    pub start: f32,    // Start time in seconds
    pub end: f32,      // End time in seconds
    pub speaker: i32,  // Speaker ID (0, 1, 2, etc.)
}

impl Diarize {
    pub fn new(
        segmentation_model: P,  // sherpa-onnx-pyannote-segmentation-3-0
        embedding_model: P,     // 3dspeaker embedding model
        config: DiarizeConfig,
    ) -> Result<Self> { ... }
    
    pub fn compute(&mut self, samples: Vec<f32>, callback: Option<...>) -> Vec<Segment>
}
```

#### Required Models (to add to download list)
| Model | Size | Purpose |
|-------|------|---------|
| sherpa-onnx-pyannote-segmentation-3-0 | ~5MB | Speaker segmentation |
| 3dspeaker_speech_eres2net_base_sv_zh-cn | ~25MB | Speaker embeddings |

#### Implementation Options
| Option | Approach | Pros | Cons |
|--------|----------|------|------|
| **Post-meeting** | Diarize after meeting ends | Accurate, simple | Not real-time |
| **Real-time chunks** | Buffer 30s, diarize periodically | Near real-time | Complex, CPU heavy |
| **Embedding clustering** | Per-segment embedding + cluster | Lightweight | Less accurate |

**Recommended:** Post-meeting diarization - after recording stops, run diarization on system audio to relabel "Guest" segments as "Speaker 1", "Speaker 2", etc.

### Files Modified

| File | Changes |
|------|---------|
| `+page.svelte` | Fixed init order, added appInitialized state, added LLM init |
| `lib.rs` | Added debug logging for KB save pipeline |
| `knowledge_base.rs` | Added debug logging in add_segment() |
| `SecondBrain.svelte` | Wired up Ask AI with state, functions, and UI |

### Architecture Update

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    INITIALIZATION SEQUENCE                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  +page.svelte::initializeApp()                                  â”‚
â”‚                                                                 â”‚
â”‚  1. User Store (SQLite)                                         â”‚
â”‚     â””â”€> initialize_user_store()                                 â”‚
â”‚                                                                 â”‚
â”‚  2. ASR Engine                                                  â”‚
â”‚     â””â”€> initialize_asr()                                        â”‚
â”‚     â””â”€> Loads Silero VAD + Zipformer models                     â”‚
â”‚                                                                 â”‚
â”‚  3. Embedding Engine                                            â”‚
â”‚     â””â”€> initialize_embeddings()                                 â”‚
â”‚     â””â”€> Loads EmbeddingGemma-300M                               â”‚
â”‚                                                                 â”‚
â”‚  4. Entity Engine (MUST be before KB)                           â”‚
â”‚     â””â”€> initialize_entities()                                   â”‚
â”‚     â””â”€> Loads GLiNER multitask model                            â”‚
â”‚                                                                 â”‚
â”‚  5. Knowledge Base (requires Entity + Embedding engines)        â”‚
â”‚     â””â”€> initialize_knowledge_base()                             â”‚
â”‚     â””â”€> Connects to SurrealDB                                   â”‚
â”‚                                                                 â”‚
â”‚  6. LLM Assistant (optional)                                    â”‚
â”‚     â””â”€> initialize_llm()                                        â”‚
â”‚     â””â”€> Connects to OpenAI-compatible API                       â”‚
â”‚                                                                 â”‚
â”‚  7. Audio Pipeline                                              â”‚
â”‚     â””â”€> audioPipeline.initialize()                              â”‚
â”‚                                                                 â”‚
â”‚  8. Event Listeners                                             â”‚
â”‚     â””â”€> recording-started, recording-stopped                    â”‚
â”‚                                                                 â”‚
â”‚  9. appInitialized = true                                       â”‚
â”‚     â””â”€> SecondBrain component now renders                       â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Ask AI Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      ASK AI FLOW (RAG)                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  1. User types question in "Ask Your Brain"                     â”‚
â”‚     â””â”€> "What action items are still open?"                     â”‚
â”‚                                                                 â”‚
â”‚  2. Frontend calls invoke("ask_assistant", { question })        â”‚
â”‚                                                                 â”‚
â”‚  3. Backend (MeetingAssistant::ask):                            â”‚
â”‚     â””â”€> Generate embedding for question                         â”‚
â”‚     â””â”€> Vector search in Knowledge Base                         â”‚
â”‚     â””â”€> Retrieve relevant segments, entities                    â”‚
â”‚     â””â”€> Build context with transcript snippets                  â”‚
â”‚                                                                 â”‚
â”‚  4. LLM API call (OpenAI-compatible):                           â”‚
â”‚     â””â”€> System prompt with tools                                â”‚
â”‚     â””â”€> Context from KB search                                  â”‚
â”‚     â””â”€> User question                                           â”‚
â”‚                                                                 â”‚
â”‚  5. Response returned to frontend                               â”‚
â”‚     â””â”€> Displayed in aiResponse area                            â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Current Tech Stack

| Component | Technology | Status |
|-----------|------------|--------|
| Desktop Framework | Tauri v2 | âœ… |
| Frontend | Svelte 5 | âœ… |
| Audio Capture | cpal | âœ… |
| VAD | Silero VAD (sherpa-rs) | âœ… |
| ASR | Zipformer (sherpa-rs) | âœ… |
| Entity Extraction | GLiNER (ort) | âœ… |
| Text Embeddings | EmbeddingGemma-300M (ort) | âœ… |
| Knowledge Base | SurrealDB | âœ… |
| User Store | SQLite | âœ… |
| LLM | OpenAI-compatible API | âœ… |
| Speaker Diarization | sherpa-rs (available, not implemented) | ğŸ”„ |

### Next Steps

1. **Test KB save flow** - Verify transcripts are being saved with debug logs
2. **Implement speaker diarization** - Use sherpa-rs::diarize for post-meeting processing
3. **Add diarization models** to download list
4. **Smart suggestions** - Show AI-generated insights on home page
5. **Export functionality** - PDF/Markdown export of meetings

---

## Session 9: Ask AI per Meeting + Delete Meetings

### Summary
Added the ability to ask AI questions about specific meetings from within the meeting detail page, and implemented meeting deletion with confirmation dialog.

### Features Implemented

#### 1. Ask AI About This Meeting
- Added `ask_about_meeting()` function in `llm_agent.rs`
  - Takes meeting title, transcript, action items, and decisions as context
  - Focused system prompt for meeting-specific questions
- Added `ask_meeting_question` Tauri command in `lib.rs`
- Updated `MeetingDetailPage.svelte`:
  - "Ask AI" button in header that toggles expandable panel
  - Input field with placeholder suggestions
  - Response display area with AI icon
  - Enter key support for quick questions
  - Loading spinner while waiting for response

#### 2. Delete Meetings~
- Added `delete_meeting()` function in `knowledge_base.rs`:
  - Deletes meeting record
  - Deletes all transcript segments
  - Deletes all action items
  - Deletes all decisions
  - Deletes entity relations (entity_relation table)
  - Deletes meeting-knowledge links
  - Deletes graph relations (mentioned_in, discussed_in edges)
- Added `delete_meeting` Tauri command in `lib.rs`
- Updated `MeetingDetailPage.svelte`:
  - Delete button (trash icon) in header
  - Confirmation modal with warning message
  - Loading state during deletion
  - Automatic navigation back after successful deletion

#### 3. UI Refresh Fix
- Updated `closeMeetingDetail()` in `SecondBrain.svelte` to reload meetings list
- Ensures deleted meetings are immediately removed from the list

### Files Modified

| File | Changes |
|------|---------|
| `llm_agent.rs` | Added `ask_about_meeting()` function |
| `knowledge_base.rs` | Added `delete_meeting()` function |
| `lib.rs` | Added `ask_meeting_question` and `delete_meeting` Tauri commands |
| `MeetingDetailPage.svelte` | Added Ask AI panel, delete button, confirmation modal |
| `SecondBrain.svelte` | Updated `closeMeetingDetail()` to reload meetings |

### Ask AI per Meeting Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               ASK AI ABOUT MEETING (Focused Context)            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  MeetingDetailPage.svelte                                       â”‚
â”‚     â”‚                                                           â”‚
â”‚     â”œâ”€> User clicks "Ask AI" button                             â”‚
â”‚     â”‚   â””â”€> Panel expands with input field                      â”‚
â”‚     â”‚                                                           â”‚
â”‚     â”œâ”€> User types question                                     â”‚
â”‚     â”‚   â””â”€> "What did John say about the deadline?"             â”‚
â”‚     â”‚                                                           â”‚
â”‚     â””â”€> invoke("ask_meeting_question", {                        â”‚
â”‚             question,                                           â”‚
â”‚             meetingTitle,                                       â”‚
â”‚             transcript: ["Speaker: text", ...],                 â”‚
â”‚             actionItems: ["action text", ...],                  â”‚
â”‚             decisions: ["decision text", ...]                   â”‚
â”‚         })                                                      â”‚
â”‚                                                                 â”‚
â”‚  Backend (MeetingAssistant::ask_about_meeting)                  â”‚
â”‚     â”‚                                                           â”‚
â”‚     â”œâ”€> Build focused system prompt                             â”‚
â”‚     â”‚   â””â”€> "You are an AI assistant answering questions        â”‚
â”‚     â”‚        about a specific meeting..."                       â”‚
â”‚     â”‚                                                           â”‚
â”‚     â”œâ”€> Format meeting context                                  â”‚
â”‚     â”‚   â””â”€> Title, transcript, action items, decisions          â”‚
â”‚     â”‚                                                           â”‚
â”‚     â””â”€> Call LLM API â†’ Return response                          â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Delete Meeting Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DELETE MEETING FLOW                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  1. User clicks trash icon in MeetingDetailPage header          â”‚
â”‚     â””â”€> showDeleteConfirm = true                                â”‚
â”‚                                                                 â”‚
â”‚  2. Confirmation modal appears with warning                     â”‚
â”‚     â””â”€> "Are you sure? This cannot be undone."                  â”‚
â”‚                                                                 â”‚
â”‚  3. User clicks "Delete Meeting"                                â”‚
â”‚     â””â”€> isDeleting = true (shows spinner)                       â”‚
â”‚     â””â”€> invoke("delete_meeting", { meetingId })                 â”‚
â”‚                                                                 â”‚
â”‚  4. Backend (KnowledgeBase::delete_meeting):                    â”‚
â”‚     â””â”€> DELETE segments WHERE meeting_id = ?                    â”‚
â”‚     â””â”€> DELETE action_items WHERE meeting_id = ?                â”‚
â”‚     â””â”€> DELETE decisions WHERE meeting_id = ?                   â”‚
â”‚     â””â”€> DELETE entity_relations WHERE meeting_id = ?            â”‚
â”‚     â””â”€> DELETE meeting_knowledge WHERE meeting_id = ?           â”‚
â”‚     â””â”€> DELETE mentioned_in edges                               â”‚
â”‚     â””â”€> DELETE discussed_in edges                               â”‚
â”‚     â””â”€> DELETE meeting record                                   â”‚
â”‚                                                                 â”‚
â”‚  5. Success â†’ onBack() called                                   â”‚
â”‚     â””â”€> SecondBrain.closeMeetingDetail()                        â”‚
â”‚     â””â”€> selectedMeetingId = null                                â”‚
â”‚     â””â”€> loadMeetings() â† Refreshes list                         â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Updated Tech Stack

| Component | Technology | Status |
|-----------|------------|--------|
| Desktop Framework | Tauri v2 | âœ… |
| Frontend | Svelte 5 | âœ… |
| Audio Capture | cpal | âœ… |
| VAD | Silero VAD (sherpa-rs) | âœ… |
| ASR | Zipformer (sherpa-rs) | âœ… |
| Entity Extraction | GLiNER (ort) | âœ… |
| Text Embeddings | EmbeddingGemma-300M (ort) | âœ… |
| Knowledge Base | SurrealDB | âœ… |
| User Store | SQLite | âœ… |
| LLM | OpenAI-compatible API | âœ… |
| Ask AI (Global) | Graph-RAG across all meetings | âœ… |
| Ask AI (Per Meeting) | Focused context for single meeting | âœ… |
| Delete Meetings | Full cascade delete | âœ… |
| Speaker Diarization | sherpa-rs (models added, engine ready) | ğŸ”„ |

---

## Session 10: SenseVoice ASR + Smart Turn v3 + Enhanced Pipeline

### Summary

Replaced ZipFormer ASR with SenseVoice (5-15x faster, emotion + audio event detection) and added Smart Turn v3 for semantic turn detection.

### What Was Built

#### 1. SenseVoice ASR (Replaced ZipFormer)

**Why:** Original ZipFormer was not accurate or fast enough.

**SenseVoice provides:**
- 5-15x faster than Whisper
- 5 languages: English, Chinese, Japanese, Korean, Cantonese
- **Emotion detection**: Neutral, Happy, Sad, Angry, Fearful, Disgusted, Surprised
- **Audio event detection**: Speech, Laughter, Applause, Music, Noise

**Files changed:**
- `models.rs` - Updated model downloads for SenseVoice
- `asr.rs` - Complete rewrite to use `sherpa_rs::sense_voice`

#### 2. Smart Turn v3 (New Module)

**Purpose:** Determines when a speaker has finished their turn using audio analysis (not just silence detection).

**How it works:**
- Uses Whisper Tiny encoder + linear classifier (8M params, ~8MB)
- Analyzes audio waveform for intonation patterns, linguistic cues
- Returns: `{ prediction: 0|1, probability: 0-1, is_complete: bool }`
- CPU inference in ~12ms

**Files created:**
- `smart_turn.rs` - New module with mel spectrogram computation + ONNX inference

#### 3. Enhanced Audio Pipeline

**New Flow:**
```
Audio (cpal)
    â†“
Silero VAD (speech detection)
    â†“
    â”œâ”€â†’ SenseVoice (transcribe + emotion + events)
    â”‚
    â””â”€â†’ Smart Turn v3 (turn completion detection)
    â†“
Speaker Diarization (post-meeting, system audio)
    â†“
Knowledge Base (store all metadata)
```

#### 4. Updated TranscriptionResult

```rust
pub struct TranscriptionResult {
    pub text: String,
    pub source: String,           // "microphone" or "system"
    pub timestamp_ms: u64,
    pub is_final: bool,
    pub language: String,         // Detected language
    pub emotion: Emotion,         // NEW: Detected emotion
    pub audio_events: Vec<AudioEvent>, // NEW: Audio events
    pub is_turn_complete: bool,   // NEW: From Smart Turn
    pub turn_confidence: f32,     // NEW: Turn confidence
}
```

#### 5. Speaker Diarization Integration

- System audio buffered during recording
- Post-meeting diarization runs on end_meeting()
- Ready for speaker relabeling (TODO: implement KB update)

#### 6. Frontend Updates

- Updated LiveSegment interface with new fields
- Live transcript now shows emotion indicators (emoji + label)
- Shows audio event badges (Laughter, Applause)
- Turn completion indicator

### Files Modified

| File | Changes |
|------|---------|
| `models.rs` | Replaced ZipFormer with SenseVoice + Smart Turn v3 models |
| `asr.rs` | Complete rewrite for SenseVoice + emotion parsing |
| `smart_turn.rs` | **NEW** - Smart Turn v3 inference with mel features |
| `lib.rs` | Added SmartTurnEngine, integrated into pipeline |
| `+page.svelte` | Added Smart Turn initialization |
| `SecondBrain.svelte` | Enhanced LiveSegment display with emotion/events |

### New Models Required

| Model | Size | Purpose |
|-------|------|---------|
| SenseVoice | ~470MB | ASR + emotion + events |
| Smart Turn v3 | ~8MB | Turn detection |

### Updated Tech Stack

| Component | Technology | Status |
|-----------|------------|--------|
| ASR | **SenseVoice** (sherpa-rs) | âœ… |
| Turn Detection | **Smart Turn v3** (ONNX) | âœ… |
| VAD | Silero VAD (sherpa-rs) | âœ… |
| Speaker Diarization | Pyannote + 3D-Speaker | âœ… Integrated |
| Entity Extraction | GLiNER (ort) | âœ… |
| Text Embeddings | EmbeddingGemma-300M (ort) | âœ… |
| Knowledge Base | SurrealDB | âœ… |
| LLM | OpenAI-compatible API | âœ… |

### Next Steps

1. **Test the new pipeline** - Verify SenseVoice + Smart Turn work correctly
2. **Implement speaker relabeling** - Update KB segments with diarization results
3. **Smart suggestions** - Show AI-generated insights on home page
4. **Export functionality** - PDF/Markdown export of meetings
5. **Meeting search** - Search within meeting transcripts

---

## Session 11: Major Feature Overhaul - Diarization, Real-time LLM, Pre-meeting Prep

**Date:** December 5, 2024

### Issues Addressed

User reported several critical issues after testing:
1. Speaker diarization buffer was empty (never filled properly)
2. GLiNER entity extraction returning 0 entities for interview content
3. No real-time LLM suggestions during meetings
4. No post-meeting LLM processing to fill highlights
5. Can't create meeting before recording (need pre-meeting prep)
6. No meeting-specific KB/document upload
7. UI showing "live" after meeting ends

### Changes Made

#### 1. Fixed System Audio Diarization

**Problem:** System audio buffer was only filled when transcription succeeded, missing audio segments.

**Solution:**
- Moved `system_audio_buffer.extend_from_slice()` BEFORE ASR processing
- Now captures ALL system audio for accurate post-meeting diarization

**Files:** `lib.rs`

```rust
// Buffer ALL system audio for post-meeting diarization (before ASR processing)
if source == "system" {
    let mut buffer = state.system_audio_buffer.lock().unwrap();
    buffer.extend_from_slice(&samples);
}
```

#### 2. Implemented Speaker Relabeling

**New method in KnowledgeBase:**

```rust
pub async fn relabel_speakers(
    &self,
    meeting_id: &str,
    diarization: &[(u64, u64, i32, String)],  // (start_ms, end_ms, speaker_id, speaker_label)
) -> Result<usize, String>
```

- Finds "Guest" segments in the meeting
- Matches segment midpoints to diarization time ranges
- Updates speaker labels to "Speaker 1", "Speaker 2", etc.

**Files:** `knowledge_base.rs`, `lib.rs`

#### 3. Fixed GLiNER Entity Extraction

**Problem:** Entity labels too business-focused, missing technical interview content.

**Solution:** Expanded entity labels for technical conversations:

```rust
pub const ENTITY_LABELS: &[&str] = &[
    // People & Organizations
    "person", "organization", "company",
    // Work & Projects
    "project", "product", "technology", "programming_language",
    // Technical concepts (for interviews)
    "algorithm", "data_structure", "concept", "problem",
    // Meeting-specific
    "action_item", "deadline", "decision", "topic", "question",
    // Measurements
    "metric", "number", "time",
    // Location
    "location",
];
```

Added new relationship types:
- `uses`, `solves`, `implements`, `requires` (for technical discussions)
- `works_at`, `asked` (for people-centric relations)

**Files:** `entities.rs`

#### 4. Added Real-time LLM Suggestions

**New structs:**

```rust
pub struct RealtimeSuggestion {
    pub insight: Option<String>,
    pub question: Option<String>,
    pub related_info: Option<String>,
}
```

**New method:** `MeetingAssistant::generate_realtime_suggestions()`
- Takes recent transcript segments
- Searches KB for related context
- Returns insight, suggested question, and related info

**New state:** `recent_transcripts: Mutex<Vec<String>>` - tracks last 10 segments

**New commands:**
- `get_realtime_suggestions` - Get AI suggestions based on recent transcript
- `clear_recent_transcripts` - Reset on meeting end

**Files:** `llm_agent.rs`, `lib.rs`

#### 5. Added Post-meeting LLM Processing

**New struct:**

```rust
pub struct MeetingHighlights {
    pub summary: Option<String>,
    pub key_topics: Vec<String>,
    pub action_items: Vec<ExtractedActionItem>,
    pub decisions: Vec<String>,
    pub highlights: Vec<String>,
    pub follow_ups: Vec<String>,
}
```

**New method:** `MeetingAssistant::process_meeting_end()`
- Analyzes full transcript after meeting ends
- Extracts structured data: summary, topics, action items, decisions

**New KB methods:**
- `add_action_item()` - Create action item with assignee/deadline
- `add_decision()` - Store decision from meeting
- `update_meeting_summary()` - Update meeting with LLM summary

**New command:** `process_meeting_highlights`

**Files:** `llm_agent.rs`, `knowledge_base.rs`, `lib.rs`

#### 6. Pre-meeting Creation (Prep Flow)

**Problem:** User couldn't prepare meeting context before recording starts.

**Solution:**

New function `createMeeting()` - creates meeting without starting recording

UI Changes:
- Added "Prepare" button next to "Start Recording"
- Meeting banner shows:
  - **Amber** state: "Preparing" - meeting exists, not recording
  - **Red** state: "Recording" - actively recording
- Prep mode shows context input and document linking

```svelte
<button onclick={createMeeting}>
  <FileText size={14} />
  Prepare
</button>
```

**Files:** `SecondBrain.svelte`

#### 7. Meeting-specific KB Upload

**New state variables:**
```typescript
let meetingContext = $state<string>("");  // Text context
let linkedSources = $state<KnowledgeSource[]>([]);
let availableSources = $state<KnowledgeSource[]>([]);
```

**New functions:**
- `loadAvailableSources()` - Load KB sources on meeting creation
- `linkSourceToMeeting()` - Link source to current meeting
- `unlinkSource()` - Remove linked source

**Prep mode UI includes:**
- Meeting context textarea
- List of linked documents with remove buttons
- Available sources to add (click to link)

**Files:** `SecondBrain.svelte`

#### 8. Fixed UI State After Meeting Ends

**Problem:** Frontend expected `recording-started` and `recording-stopped` events but backend never emitted them.

**Solution:** Added event emissions:

```rust
// In start_recording:
let _ = app.emit("recording-started", ());

// In stop_recording:
fn stop_recording(state: tauri::State<AppState>, app: tauri::AppHandle) -> Result<(), String> {
    // ...
    let _ = app.emit("recording-stopped", ());
    // ...
}
```

**Files:** `lib.rs`

### Files Modified

| File | Changes |
|------|---------|
| `lib.rs` | System audio buffering fix, speaker relabeling, real-time suggestions, event emissions |
| `knowledge_base.rs` | `relabel_speakers()`, `add_action_item()`, `add_decision()`, `update_meeting_summary()` |
| `llm_agent.rs` | `RealtimeSuggestion`, `MeetingHighlights`, `generate_realtime_suggestions()`, `process_meeting_end()` |
| `entities.rs` | Expanded entity labels + relationship schema for technical content |
| `SecondBrain.svelte` | Prep flow, meeting context, document linking, UI state fixes |

### New Tauri Commands

| Command | Purpose |
|---------|---------|
| `get_realtime_suggestions` | Get AI insights during meeting |
| `clear_recent_transcripts` | Reset transcript buffer |
| `process_meeting_highlights` | Post-meeting LLM analysis |

### UI Flow Changes

**Before:**
```
Enter title â†’ Start Recording â†’ Record â†’ End Meeting
```

**After:**
```
Enter title â†’ Prepare (optional) â†’ Add context/docs â†’ Start Recording â†’ Record â†’ End Meeting â†’ Auto-process highlights
```

### Speaker Attribution Logic

| Audio Source | Speaker Label | Diarization |
|--------------|---------------|-------------|
| Microphone | "You" | Not needed |
| System Audio | "Guest" â†’ "Speaker 1/2/3..." | Post-meeting diarization relabels |

### Next Steps

1. Test complete flow with system audio from real calls
2. Connect real-time suggestions to UI (poll or WebSocket)
3. Display meeting highlights after processing
4. Add UI for viewing/editing extracted action items
5. Implement follow-up reminders based on extracted items

---

## Session 14 - UI/UX Overhaul & Dashboard Real Data

### Issues Fixed

#### 1. Insights Page Not Showing (Critical Bug)
**Problem:** Insights view was incorrectly nested inside the Notes view block - missing `{:else if}`.

**Fix:** Added proper condition:
```svelte
{:else if activeView === 'insights'}
  <!-- INSIGHTS VIEW -->
```

**Files:** `SecondBrain.svelte`

#### 2. Meeting Prep - No Dedicated Page
**Problem:** Prep mode was inline in the dashboard, couldn't add context or cancel properly.

**Solution:** Created `MeetingPrepPage.svelte` - full dedicated page with:
- Meeting title editing
- Context/agenda textarea with description
- Document linking from Knowledge Base (with picker modal)
- Cancel Meeting button (deletes the meeting)
- Start Recording button

```svelte
<MeetingPrepPage
  meetingId={currentMeetingId}
  initialTitle={meetingTitle}
  onCancel={cancelMeetingPrep}
  onStartRecording={startRecordingFromPrep}
/>
```

**Files:** `MeetingPrepPage.svelte` (new), `SecondBrain.svelte`

#### 3. Dashboard Mock Data Removed
**Problem:** Dashboard showed hardcoded fake data for "Today's Agenda" and "Insights".

**Solution:** Replaced with real data from database:

**New interfaces:**
```typescript
interface ActionItem {
  id: string;
  text: string;
  assignee: string | null;
  deadline: string | null;
  status: string;
  meeting_title: string;
  meeting_id: string;
  created_at: number;
}

interface Decision {
  id: string;
  text: string;
  meeting_title: string;
  meeting_id: string;
  created_at: number;
}

interface EntityCount {
  label: string;
  count: number;
}
```

**New state:**
```typescript
let actionItems = $state<ActionItem[]>([]);
let decisions = $state<Decision[]>([]);
let entityCounts = $state<EntityCount[]>([]);
let totalSegments = $state(0);
```

**Computed derived data:**
```typescript
let upcomingItems = $derived(() => {
  // Build from actionItems with deadlines
  // Sort by urgency (overdue, today, tomorrow, etc.)
});

let insights = $derived(() => {
  // Generate insights from real data:
  // - Open action items count
  // - Recent decisions
  // - Top entity types
  // - Meeting stats for week
  // - Empty state when no data
});
```

**Files:** `SecondBrain.svelte`

#### 4. Ask AI Bar at Bottom
**Problem:** AI assistant input was buried at bottom of dashboard, not user-friendly.

**Solution:** Moved to top of dashboard, right after header:
- More prominent purple/indigo styling
- Full-width input with "Ask AI" button
- Response shows directly below in styled card

```svelte
<header class="pb-6 border-b border-[#2C2C2C]">
  <div class="flex items-end justify-between mb-4">...</div>
  <!-- Ask AI - Primary action at top -->
  <div class="bg-[#252525] rounded-lg p-1.5 pl-4 flex items-center gap-3 ring-1 ring-[#333333] focus-within:ring-indigo-500/50">
    <Sparkles size={16} class="text-indigo-400" />
    <input placeholder="Ask anything about your meetings..." />
    <button class="bg-indigo-500 hover:bg-indigo-400">Ask AI</button>
  </div>
</header>
```

**Files:** `SecondBrain.svelte`

#### 5. Insights Page Empty State
**Problem:** Insights page was completely empty with no meetings.

**Solution:** Redesigned with:
- Stats summary cards (Meetings, Action Items, Decisions, Segments)
- Action items section with empty state
- Decisions section with empty state
- Patterns section using computed insights
- Proper messaging: "Record meetings to discover patterns..."

```svelte
<section class="grid grid-cols-4 gap-4">
  <div class="bg-[#252525] rounded-lg p-4 text-center">
    <p class="text-3xl font-bold">{meetings.length}</p>
    <p class="text-xs text-[#9B9B9B]">Meetings</p>
  </div>
  <!-- ... more stat cards -->
</section>
```

**Files:** `SecondBrain.svelte`

### Backend Commands Added

#### New Tauri Commands (`lib.rs`)

```rust
// Get ALL action items across all meetings
#[tauri::command]
async fn get_all_action_items(
    state: tauri::State<'_, AppState>,
    limit: Option<usize>,
) -> Result<Vec<serde_json::Value>, String>

// Get ALL decisions across all meetings
#[tauri::command]
async fn get_all_decisions(
    state: tauri::State<'_, AppState>,
    limit: Option<usize>,
) -> Result<Vec<serde_json::Value>, String>

// Get overall knowledge base statistics
#[tauri::command]
async fn get_knowledge_stats(
    state: tauri::State<'_, AppState>,
) -> Result<serde_json::Value, String>
```

#### New KnowledgeBase Methods (`knowledge_base.rs`)

```rust
/// Get ALL action items with meeting titles
pub async fn get_all_action_items(&self, limit: usize) -> Result<Vec<serde_json::Value>, String> {
    self.db.query(r#"
        SELECT id, text, assignee, deadline, status, meeting_id,
               (SELECT title FROM meeting WHERE id = $parent.meeting_id)[0].title AS meeting_title,
               created_at
        FROM action_item
        ORDER BY created_at DESC
        LIMIT $limit
    "#).bind(("limit", limit)).await
}

/// Get ALL decisions with meeting titles
pub async fn get_all_decisions(&self, limit: usize) -> Result<Vec<serde_json::Value>, String> {
    self.db.query(r#"
        SELECT id, text, meeting_id,
               (SELECT title FROM meeting WHERE id = $parent.meeting_id)[0].title AS meeting_title,
               created_at
        FROM decision
        ORDER BY created_at DESC
        LIMIT $limit
    "#).bind(("limit", limit)).await
}

/// Get global statistics
pub async fn get_global_stats(&self) -> Result<serde_json::Value, String> {
    // Returns: { total_segments: u64, entity_counts: [{label, count}] }
}
```

### Files Modified

| File | Changes |
|------|---------|
| `SecondBrain.svelte` | Fixed Insights nesting, moved Ask AI to top, real data loading, derived insights |
| `MeetingPrepPage.svelte` | **New** - dedicated meeting prep page |
| `lib.rs` | Added `get_all_action_items`, `get_all_decisions`, `get_knowledge_stats` commands |
| `knowledge_base.rs` | Added corresponding database query methods |

### UI Flow Changes

**Dashboard:**
- Ask AI bar now at top (primary action)
- "Action Items" section shows real items from DB
- "Insights" section shows computed patterns from real data
- Empty states with helpful messaging

**Insights Page:**
- Stats cards at top (4 metrics)
- Action items list with status/assignee/deadline
- Decisions list with meeting source
- Patterns section with real computed insights

**Meeting Prep Flow:**
```
Enter title â†’ Click "Prepare" â†’ Full page prep UI â†’ Add context â†’ Link docs â†’ Start Recording
                                      â†“
                              Click "Cancel" â†’ Deletes meeting, returns to dashboard
```

### Empty State Messaging

| Section | Empty State Message |
|---------|---------------------|
| Action Items | "No action items yet - Items from meetings will appear here" |
| Decisions | "No decisions recorded - Key decisions from meetings will appear here" |
| Insights | "Getting Started - Record your first meeting to see insights" |
| Insights Page Header | "Record meetings to discover patterns and track action items" |

---

## Session 16 - Agent Queue System & Audio Debug

### What Was Built

#### 1. Agent Queue System (`agent_queue.rs`)
Created an in-memory job queue system using tokio mpsc channels for AI agents:

```rust
pub enum AgentJob {
    RealtimeSuggestions { meeting_id, recent_transcripts, context, response_tx },
    AnswerQuestion { question, context, response_tx },
    PostMeetingHighlights { meeting_id, response_tx },
    EntityExtraction { text, source, timestamp_ms, response_tx },
    Shutdown,
}

pub struct AgentQueue {
    job_tx: mpsc::Sender<AgentJob>,
    stats: Arc<RwLock<QueueStats>>,
}
```

#### 2. Agent Workers (`agent_workers.rs`)
Created worker module to process different job types:
- `process_realtime_suggestions` - During meetings
- `process_answer_question` - User questions with KB context
- `process_meeting_highlights` - Post-meeting extraction
- `process_entity_extraction` - GLiNER NER

#### 3. Queue Commands in lib.rs
New Tauri commands for queue operations:
- `initialize_agent_queue` - Initialize queue system
- `get_queue_stats` - Get pending/processed/failed counts
- `queue_ask_question` - Submit question to queue
- `queue_realtime_suggestions` - Get suggestions during meeting
- `queue_meeting_highlights` - Process meeting end
- `queue_entity_extraction` - Extract entities from text

Note: Currently processes inline (not background workers) for simplicity.

#### 4. System Audio Debug Logging
Added RMS level logging to diagnose why system audio shows as "microphone":

```rust
// Calculate RMS level for debugging
let rms: f32 = (samples.iter().map(|s| s * s).sum::<f32>() / samples.len() as f32).sqrt();

println!("[ASR] SYSTEM audio chunk #{}: {} samples at {}Hz, RMS={:.6} ({}dB)",
    system_chunk_count, samples.len(), sample_rate, rms,
    if rms > 0.0 { (20.0 * rms.log10()) as i32 } else { -100 });
```

#### 5. Stereo-to-Mono Conversion
Fixed issue where BlackHole stereo audio wasn't being processed correctly:

```rust
fn stereo_to_mono(samples: &[f32], channels: u16) -> Vec<f32> {
    if channels <= 1 { return samples.to_vec(); }
    let channels = channels as usize;
    let num_frames = samples.len() / channels;
    let mut mono = Vec::with_capacity(num_frames);
    for frame in 0..num_frames {
        let mut sum = 0.0f32;
        for ch in 0..channels {
            sum += samples[frame * channels + ch];
        }
        mono.push(sum / channels as f32);
    }
    mono
}
```

#### 6. Real-time Suggestions UI
Added polling mechanism and display during recording:

```typescript
// Poll every 10 seconds during recording
suggestionPollInterval = setInterval(() => {
    if (liveTranscript.length > 0) {
        fetchRealtimeSuggestions();
    }
}, 10000);
```

UI shows:
- **Insight** (amber) - Key insight about discussion
- **Ask** (green) - Suggested question to ask
- **Related** (blue) - Info from knowledge base

#### 7. Post-Meeting Processing
Added automatic highlights extraction when meeting ends:

```typescript
async function processPostMeetingHighlights(meetingId: string) {
    const highlights = await invoke("process_meeting_highlights", { meetingId });
    // Reloads action items, decisions, meetings after processing
}
```

### Architecture

```
Agent Queue System
â”œâ”€â”€ AgentQueue (mpsc channel + stats)
â”‚   â””â”€â”€ submit(job) â†’ job_tx.send()
â”œâ”€â”€ Job Types
â”‚   â”œâ”€â”€ RealtimeSuggestions â†’ LLM + KB
â”‚   â”œâ”€â”€ AnswerQuestion â†’ LLM + KB
â”‚   â”œâ”€â”€ PostMeetingHighlights â†’ LLM
â”‚   â””â”€â”€ EntityExtraction â†’ GLiNER
â””â”€â”€ Commands (inline processing)
    â””â”€â”€ queue_* functions process immediately
```

### Files Modified/Created

| File | Changes |
|------|---------|
| `agent_queue.rs` | **New** - Job queue with mpsc channels |
| `agent_workers.rs` | **New** - Job processing functions |
| `lib.rs` | Added queue commands, debug logging, stereo conversion |
| `asr.rs` | Added VAD speech detection logging |
| `SecondBrain.svelte` | Added realtime suggestions UI, post-meeting processing |

### GLiNER Status

Using `gline-rs = "1"` with `orp = "0.9"` (ONNX Runtime Pipeline). Need ONNX model files:
- `gliner-tokenizer.json`
- `gliner-model.onnx`

The crate supports various GLiNER models converted to ONNX format.

### Next Steps

1. **Background Workers** - Currently inline processing; can add true async workers
2. **BlackHole Config** - User needs to configure multi-output device for system audio
3. **Queue Persistence** - Could use SurrealDB for persistent job queue
